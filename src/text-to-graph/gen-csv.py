import pandas as pd
from datetime import date

# Define a minimal, reproducible evaluation matrix + stable results schema
columns = [
    "run_id",
    "benchmark_name",
    "benchmark_repo",
    "benchmark_commit_or_tag",
    "benchmark_version_notes",
    "dataset_name",
    "dataset_subset",
    "split",
    "ontology_set",
    "task_definition",
    "prompt_template_id",
    "prompt_template_url",
    "prompting_mode",
    "examples_in_prompt",
    "output_format",
    "parser_version",
    "evaluation_script",
    "evaluation_script_version",
    "decoding_temperature",
    "top_p",
    "max_output_tokens",
    "random_seed",
    "model_provider",
    "model_name",
    "model_version_or_date",
    "access_method",
    "api_base_or_runtime",
    "hardware_or_region",
    "run_date",
    "primary_metric",
    "primary_metric_value",
    "secondary_metrics_json",
    "num_items_evaluated",
    "cost_estimate_usd",
    "latency_p50_ms",
    "notes",
    "source_type",
    "source_title",
    "source_citation",
    "source_url",
    "chart_series",
    "chart_label",
    "chart_metric",
    "chart_value",
    "ref_key",
    "ref_text",
    "ref_url"
]

# Create example rows (values are placeholders; meant as a template you can fill from sources)
today = date.today().isoformat()

rows = [
    {
        "run_id": "T2KGB-EXAMPLE-0001",
        "benchmark_name": "Text2KGBench",
        "benchmark_repo": "https://github.com/cenguix/Text2KGBench",
        "benchmark_commit_or_tag": "REPLACE_WITH_TAG_OR_COMMIT",
        "benchmark_version_notes": "Use a pinned commit/tag for full reproducibility.",
        "dataset_name": "DBpedia-WebNLG (as used in Text2KGBench)",
        "dataset_subset": "default",
        "split": "test",
        "ontology_set": "WebNLG ontologies (19)",
        "task_definition": "Ontology-driven text-to-KG triple extraction/generation; evaluate faithfulness + ontology compliance.",
        "prompt_template_id": "prompt-v1-minimal-json",
        "prompt_template_url": "REPLACE_WITH_YOUR_REPO_LINK_OR_DOC",
        "prompting_mode": "zero-shot",
        "examples_in_prompt": 0,
        "output_format": "JSON triples: [{subject,predicate,object}]",
        "parser_version": "parser-v1",
        "evaluation_script": "Text2KGBench eval_metrics",
        "evaluation_script_version": "REPLACE_WITH_TAG_OR_COMMIT",
        "decoding_temperature": 0.0,
        "top_p": 1.0,
        "max_output_tokens": 1024,
        "random_seed": 42,
        "model_provider": "OpenAI",
        "model_name": "REPLACE_WITH_MODEL (e.g., gpt-4.1)",
        "model_version_or_date": "REPLACE_WITH_VERSION_OR_DATE",
        "access_method": "API",
        "api_base_or_runtime": "REPLACE (e.g., OpenAI Responses API)",
        "hardware_or_region": "us-central (example)",
        "run_date": today,
        "primary_metric": "F1_overall",
        "primary_metric_value": "",
        "secondary_metrics_json": "",
        "num_items_evaluated": "",
        "cost_estimate_usd": "",
        "latency_p50_ms": "",
        "notes": "Template row—fill with values from a published source or your own run.",
        "source_type": "paper_or_report",
        "source_title": "Text2KGBench: Benchmark for Ontology-Driven Text-to-KG",
        "source_citation": "Guix et al., 2023 (Text2KGBench)",
        "source_url": "https://arxiv.org/abs/2308.02357",
        "chart_series": "OpenAI",
        "chart_label": "OpenAI (example model)",
        "chart_metric": "F1_overall",
        "chart_value": "",
        "ref_key": "T2KGB",
        "ref_text": "Guix et al., 2023. Text2KGBench: Benchmark for Ontology-Driven Text-to-KG.",
        "ref_url": "https://arxiv.org/abs/2308.02357"
    },
    {
        "run_id": "T2KGB-EXAMPLE-0002",
        "benchmark_name": "Text2KGBench",
        "benchmark_repo": "https://github.com/cenguix/Text2KGBench",
        "benchmark_commit_or_tag": "REPLACE_WITH_TAG_OR_COMMIT",
        "benchmark_version_notes": "Same pinned benchmark version as other rows.",
        "dataset_name": "Wikidata-TekGen (as used in Text2KGBench)",
        "dataset_subset": "default",
        "split": "test",
        "ontology_set": "TekGen ontologies (10)",
        "task_definition": "Ontology-driven text-to-KG triple extraction/generation; evaluate factuality + ontology compliance.",
        "prompt_template_id": "prompt-v1-minimal-json",
        "prompt_template_url": "REPLACE_WITH_YOUR_REPO_LINK_OR_DOC",
        "prompting_mode": "zero-shot",
        "examples_in_prompt": 0,
        "output_format": "JSON triples: [{subject,predicate,object}]",
        "parser_version": "parser-v1",
        "evaluation_script": "Text2KGBench eval_metrics",
        "evaluation_script_version": "REPLACE_WITH_TAG_OR_COMMIT",
        "decoding_temperature": 0.0,
        "top_p": 1.0,
        "max_output_tokens": 1024,
        "random_seed": 42,
        "model_provider": "Google",
        "model_name": "REPLACE_WITH_MODEL (e.g., gemini-2.5-pro)",
        "model_version_or_date": "REPLACE_WITH_VERSION_OR_DATE",
        "access_method": "API",
        "api_base_or_runtime": "REPLACE (e.g., Google AI Studio / Vertex AI)",
        "hardware_or_region": "us-central (example)",
        "run_date": today,
        "primary_metric": "F1_overall",
        "primary_metric_value": "",
        "secondary_metrics_json": "",
        "num_items_evaluated": "",
        "cost_estimate_usd": "",
        "latency_p50_ms": "",
        "notes": "Template row—keep prompt+decoding fixed across models for objective comparison.",
        "source_type": "paper_or_report",
        "source_title": "Text2KGBench: Benchmark for Ontology-Driven Text-to-KG",
        "source_citation": "Guix et al., 2023 (Text2KGBench)",
        "source_url": "https://arxiv.org/abs/2308.02357",
        "chart_series": "Google",
        "chart_label": "Google (example model)",
        "chart_metric": "F1_overall",
        "chart_value": "",
        "ref_key": "T2KGB",
        "ref_text": "Guix et al., 2023. Text2KGBench: Benchmark for Ontology-Driven Text-to-KG.",
        "ref_url": "https://arxiv.org/abs/2308.02357"
    },
    {
        "run_id": "T2KGB-EXAMPLE-0003",
        "benchmark_name": "Text2KGBench",
        "benchmark_repo": "https://github.com/cenguix/Text2KGBench",
        "benchmark_commit_or_tag": "REPLACE_WITH_TAG_OR_COMMIT",
        "benchmark_version_notes": "Same pinned benchmark version as other rows.",
        "dataset_name": "DBpedia-WebNLG (as used in Text2KGBench)",
        "dataset_subset": "default",
        "split": "test",
        "ontology_set": "WebNLG ontologies (19)",
        "task_definition": "Ontology-driven text-to-KG triple extraction/generation; evaluate faithfulness + ontology compliance.",
        "prompt_template_id": "prompt-v1-minimal-json",
        "prompt_template_url": "REPLACE_WITH_YOUR_REPO_LINK_OR_DOC",
        "prompting_mode": "zero-shot",
        "examples_in_prompt": 0,
        "output_format": "JSON triples: [{subject,predicate,object}]",
        "parser_version": "parser-v1",
        "evaluation_script": "Text2KGBench eval_metrics",
        "evaluation_script_version": "REPLACE_WITH_TAG_OR_COMMIT",
        "decoding_temperature": 0.0,
        "top_p": 1.0,
        "max_output_tokens": 1024,
        "random_seed": 42,
        "model_provider": "Anthropic",
        "model_name": "REPLACE_WITH_MODEL (e.g., claude-4-sonnet)",
        "model_version_or_date": "REPLACE_WITH_VERSION_OR_DATE",
        "access_method": "API",
        "api_base_or_runtime": "REPLACE (e.g., Anthropic Messages API)",
        "hardware_or_region": "us-central (example)",
        "run_date": today,
        "primary_metric": "F1_overall",
        "primary_metric_value": "",
        "secondary_metrics_json": "",
        "num_items_evaluated": "",
        "cost_estimate_usd": "",
        "latency_p50_ms": "",
        "notes": "Template row—fill with reported results from a paper/blog or internal measurement.",
        "source_type": "paper_or_report",
        "source_title": "Text2KGBench: Benchmark for Ontology-Driven Text-to-KG",
        "source_citation": "Guix et al., 2023 (Text2KGBench)",
        "source_url": "https://arxiv.org/abs/2308.02357",
        "chart_series": "Anthropic",
        "chart_label": "Anthropic (example model)",
        "chart_metric": "F1_overall",
        "chart_value": "",
        "ref_key": "T2KGB",
        "ref_text": "Guix et al., 2023. Text2KGBench: Benchmark for Ontology-Driven Text-to-KG.",
        "ref_url": "https://arxiv.org/abs/2308.02357"
    },
    {
        "run_id": "T2KGB-BASELINE-0004",
        "benchmark_name": "Text2KGBench",
        "benchmark_repo": "https://github.com/cenguix/Text2KGBench",
        "benchmark_commit_or_tag": "REPLACE_WITH_TAG_OR_COMMIT",
        "benchmark_version_notes": "Baseline row from official repo artifacts (if you choose to include).",
        "dataset_name": "DBpedia-WebNLG (as used in Text2KGBench)",
        "dataset_subset": "default",
        "split": "test",
        "ontology_set": "WebNLG ontologies (19)",
        "task_definition": "Ontology-driven text-to-KG triple extraction/generation; evaluate faithfulness + ontology compliance.",
        "prompt_template_id": "baseline-prompt",
        "prompt_template_url": "REPLACE_WITH_BASELINE_PROMPT_LINK",
        "prompting_mode": "baseline",
        "examples_in_prompt": "",
        "output_format": "REPLACE_WITH_BASELINE_OUTPUT_FORMAT",
        "parser_version": "baseline-parser",
        "evaluation_script": "Text2KGBench eval_metrics",
        "evaluation_script_version": "REPLACE_WITH_TAG_OR_COMMIT",
        "decoding_temperature": "",
        "top_p": "",
        "max_output_tokens": "",
        "random_seed": "",
        "model_provider": "Open-weights baseline",
        "model_name": "Vicuna-13B (example)",
        "model_version_or_date": "",
        "access_method": "local",
        "api_base_or_runtime": "REPLACE (e.g., HF Transformers)",
        "hardware_or_region": "REPLACE (GPU type)",
        "run_date": "",
        "primary_metric": "F1_overall",
        "primary_metric_value": "",
        "secondary_metrics_json": "",
        "num_items_evaluated": "",
        "cost_estimate_usd": "",
        "latency_p50_ms": "",
        "notes": "Optional baseline placeholder—fill from official repo artifacts if desired.",
        "source_type": "repo_artifact",
        "source_title": "Text2KGBench GitHub baselines",
        "source_citation": "Text2KGBench repo: baseline metrics artifacts",
        "source_url": "https://github.com/cenguix/Text2KGBench",
        "chart_series": "Baseline",
        "chart_label": "Vicuna-13B (baseline)",
        "chart_metric": "F1_overall",
        "chart_value": "",
        "ref_key": "T2KGB-REPO",
        "ref_text": "Text2KGBench GitHub repository baseline artifacts.",
        "ref_url": "https://github.com/cenguix/Text2KGBench"
    }
]

df = pd.DataFrame(rows, columns=columns)

# Save CSV
filepath = "/mnt/data/text2kgbench_eval_matrix_template.csv"
df.to_csv(filepath, index=False)

filepath

run_id,benchmark_name,benchmark_repo,benchmark_commit_or_tag,benchmark_version_notes,dataset_name,dataset_subset,split,ontology_set,task_definition,prompt_template_id,prompt_template_url,prompting_mode,examples_in_prompt,output_format,parser_version,evaluation_script,evaluation_script_version,decoding_temperature,top_p,max_output_tokens,random_seed,model_provider,model_name,model_version_or_date,access_method,api_base_or_runtime,hardware_or_region,run_date,primary_metric,primary_metric_value,secondary_metrics_json,num_items_evaluated,cost_estimate_usd,latency_p50_ms,notes,source_type,source_title,source_citation,source_url,chart_series,chart_label,chart_metric,chart_value,ref_key,ref_text,ref_url
T2KGB-ISWC2023-DBPEDIA-VICUNA,Text2KGBench,https://github.com/cenguix/Text2KGBench,ISWC2023-paper-table2,Values copied from ISWC 2023 paper Table 2 (average across ontologies).,DBpedia-WebNLG,default,test,19 ontologies,"Ontology-guided triple extraction; compute Fact Extraction P/R/F1, Ontology Conformance (OC), Hallucination (SH/RH/OH).",auto-generated prompts (paper),https://iswc2023.semanticweb.org/wp-content/uploads/2023/11/142660244.pdf,paper-setting,,"relation(subject, object) notation (paper)",paper-setting,Text2KGBench eval_metrics,paper-setting,,,,,Open-weights baseline,Vicuna-13B,as used in ISWC 2023 paper,local,FastChat (paper references),,2023-11-??,F1_fact_extraction,0.3,"{""P"": 0.34, ""R"": 0.27, ""F1"": 0.3, ""OC"": 0.93, ""SH"": 0.12, ""RH"": 0.07, ""OH"": 0.28}",4860,,,DBpedia-WebNLG aggregate row from Table 2.,paper,Text2KGBench (ISWC 2023),"Mihindukulasooriya et al., ISWC 2023, Table 2",https://iswc2023.semanticweb.org/wp-content/uploads/2023/11/142660244.pdf,Baseline (Text2KGBench),Vicuna-13B (Text2KGBench DBpedia),F1_fact_extraction,0.3,ISWC2023-T2KGB,"Mihindukulasooriya et al., 2023. Text2KGBench (ISWC 2023). Table 2 aggregate metrics.",https://iswc2023.semanticweb.org/wp-content/uploads/2023/11/142660244.pdf
T2KGB-ISWC2023-TEKGEN-VICUNA-ALL,Text2KGBench,https://github.com/cenguix/Text2KGBench,ISWC2023-paper-table2,Values copied from ISWC 2023 paper Table 2 (average across ontologies).,Wikidata-TekGen,All,test,10 ontologies,"Ontology-guided triple extraction; compute Fact Extraction P/R/F1, Ontology Conformance (OC), Hallucination (SH/RH/OH).",auto-generated prompts (paper),https://iswc2023.semanticweb.org/wp-content/uploads/2023/11/142660244.pdf,paper-setting,,"relation(subject, object) notation (paper)",paper-setting,Text2KGBench eval_metrics,paper-setting,,,,,Open-weights baseline,Vicuna-13B,as used in ISWC 2023 paper,local,FastChat (paper references),,2023-11-??,F1_fact_extraction,0.35,"{""P"": 0.38, ""R"": 0.34, ""F1"": 0.35, ""OC"": 0.83, ""SH"": 0.17, ""RH"": 0.17, ""OH"": 0.17}",13474,,,Wikidata-TekGen Vicuna 'All' aggregate row from Table 2.,paper,Text2KGBench (ISWC 2023),"Mihindukulasooriya et al., ISWC 2023, Table 2",https://iswc2023.semanticweb.org/wp-content/uploads/2023/11/142660244.pdf,Baseline (Text2KGBench),Vicuna-13B (Text2KGBench TekGen-All),F1_fact_extraction,0.35,ISWC2023-T2KGB,"Mihindukulasooriya et al., 2023. Text2KGBench (ISWC 2023). Table 2 aggregate metrics.",https://iswc2023.semanticweb.org/wp-content/uploads/2023/11/142660244.pdf
T2KGBL-CEUR2025-OPENAI-GPT41,Text2KGBench-LettrIA,https://ceur-ws.org/Vol-4041/paper3.pdf,CEUR-Vol-4041-paper3-Table1,Values copied from CEUR-WS paper Table 1 (Full benchmark; closed models; 1-shot). Dataset available upon request per paper.,DBpedia-WebNLG (re-annotated),Full benchmark,test,19 refined ontologies,Text2KG extraction with ontology; evaluate macro F1 plus hallucination/fidelity/validity/latency/cost.,1-shot (paper),https://ceur-ws.org/Vol-4041/paper3.pdf,one-shot,1,JSON (paper),paper-setting,LettrIA evaluation suite (Table 1),paper-setting,,,,,OpenAI,gpt-4.1-2025-04-14,2025-04-14,API,OpenAI API,,2025-??-??,F1_overall,0.6472,"{""F1_entities"": 0.8742, ""F1_attributes"": 0.863, ""F1_properties"": 0.6565, ""halluc_types"": 0.0014, ""halluc_relations"": 0.0004, ""halluc_properties"": 0.0146, ""respect_relations"": 0.9798, ""respect_properties"": 0.9843, ""valid_outputs_pct"": 97.27, ""latency_s"": 3.9289, ""cost_usd"": 0.0058}",4860,0.0058,,"Closed models, 1-shot, full benchmark as reported in Table 1.",paper,Text2KGBench-LettrIA (CEUR-WS @ ISWC 2025),"Plu et al., 2025, Table 1",https://ceur-ws.org/Vol-4041/paper3.pdf,OpenAI,GPT-4.1 (Text2KGBench-LettrIA),F1_overall,0.6472,CEUR2025-T2KGBL,"Plu et al., 2025. Text2KGBench-LettrIA (CEUR-WS). Table 1 closed-model 1-shot results.",https://ceur-ws.org/Vol-4041/paper3.pdf
T2KGBL-CEUR2025-GOOGLE-GEMINI25PRO,Text2KGBench-LettrIA,https://ceur-ws.org/Vol-4041/paper3.pdf,CEUR-Vol-4041-paper3-Table1,Values copied from CEUR-WS paper Table 1 (Full benchmark; closed models; 1-shot). Dataset available upon request per paper.,DBpedia-WebNLG (re-annotated),Full benchmark,test,19 refined ontologies,Text2KG extraction with ontology; evaluate macro F1 plus hallucination/fidelity/validity/latency/cost.,1-shot (paper),https://ceur-ws.org/Vol-4041/paper3.pdf,one-shot,1,JSON (paper),paper-setting,LettrIA evaluation suite (Table 1),paper-setting,,,,,Google,gemini-2.5-pro,,API,Google API,,2025-??-??,F1_overall,0.6595,"{""F1_entities"": 0.8762, ""F1_attributes"": 0.8627, ""F1_properties"": 0.7076, ""halluc_types"": 0.0014, ""halluc_relations"": 0.0, ""halluc_properties"": 0.0022, ""respect_relations"": 0.9925, ""respect_properties"": 0.9966, ""valid_outputs_pct"": 99.8, ""latency_s"": 3.9886, ""cost_usd"": 0.005}",4860,0.005,,"Closed models, 1-shot, full benchmark as reported in Table 1.",paper,Text2KGBench-LettrIA (CEUR-WS @ ISWC 2025),"Plu et al., 2025, Table 1",https://ceur-ws.org/Vol-4041/paper3.pdf,Google,Gemini 2.5 Pro (Text2KGBench-LettrIA),F1_overall,0.6595,CEUR2025-T2KGBL,"Plu et al., 2025. Text2KGBench-LettrIA (CEUR-WS). Table 1 closed-model 1-shot results.",https://ceur-ws.org/Vol-4041/paper3.pdf
T2KGBL-CEUR2025-ANTHROPIC-CLAUDE4SONNET,Text2KGBench-LettrIA,https://ceur-ws.org/Vol-4041/paper3.pdf,CEUR-Vol-4041-paper3-Table1,Values copied from CEUR-WS paper Table 1 (Full benchmark; closed models; 1-shot). Dataset available upon request per paper.,DBpedia-WebNLG (re-annotated),Full benchmark,test,19 refined ontologies,Text2KG extraction with ontology; evaluate macro F1 plus hallucination/fidelity/validity/latency/cost.,1-shot (paper),https://ceur-ws.org/Vol-4041/paper3.pdf,one-shot,1,JSON (paper),paper-setting,LettrIA evaluation suite (Table 1),paper-setting,,,,,Anthropic,claude-sonnet-4,,API,Anthropic API,,2025-??-??,F1_overall,0.6487,"{""F1_entities"": 0.8657, ""F1_attributes"": 0.8498, ""F1_properties"": 0.7126, ""halluc_types"": 0.0011, ""halluc_relations"": 0.0, ""halluc_properties"": 0.0065, ""respect_relations"": 0.9908, ""respect_properties"": 0.9848, ""valid_outputs_pct"": 99.35, ""latency_s"": 10.4307, ""cost_usd"": 0.0111}",4860,0.0111,,"Closed models, 1-shot, full benchmark as reported in Table 1.",paper,Text2KGBench-LettrIA (CEUR-WS @ ISWC 2025),"Plu et al., 2025, Table 1",https://ceur-ws.org/Vol-4041/paper3.pdf,Anthropic,Claude Sonnet 4 (Text2KGBench-LettrIA),F1_overall,0.6487,CEUR2025-T2KGBL,"Plu et al., 2025. Text2KGBench-LettrIA (CEUR-WS). Table 1 closed-model 1-shot results.",https://ceur-ws.org/Vol-4041/paper3.pdf

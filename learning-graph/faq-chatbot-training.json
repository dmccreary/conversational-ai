{
  "faq_version": "1.0",
  "generated_date": "2025-11-15",
  "source_textbook": "Conversational AI",
  "total_questions": 85,
  "categories": [
    "Getting Started",
    "Core Concepts",
    "Technical Detail Questions",
    "Common Challenge Questions",
    "Best Practice Questions",
    "Advanced Topics"
  ],
  "bloom_levels": {
    "Remember": 15,
    "Understand": 28,
    "Apply": 21,
    "Analyze": 13,
    "Evaluate": 6,
    "Create": 2
  },
  "questions": [
    {
      "id": "faq-001",
      "category": "Getting Started",
      "question": "What is this course about?",
      "answer": "This course teaches you to build intelligent chatbots and conversational AI systems from the ground up. You'll start with basic keyword search and progress to cutting-edge GraphRAG systems that power modern conversational AI. The course covers everything from search technologies and natural language processing to large language models, embeddings, vector databases, and production deployment with security and privacy considerations.",
      "bloom_level": "Understand",
      "difficulty": "easy",
      "concepts": ["Artificial Intelligence", "Conversational Agent", "Course Overview"],
      "keywords": ["course", "chatbot", "conversational AI", "GraphRAG", "overview"],
      "source_links": ["docs/course-description.md"],
      "has_example": false,
      "word_count": 52
    },
    {
      "id": "faq-002",
      "category": "Getting Started",
      "question": "Who is this course for?",
      "answer": "This course is designed for college sophomores with basic programming knowledge. You'll need basic Python skills (functions and loops), comfort with terminal commands, VSCode IDE, and a GitHub account. Non-CS majors are welcomeâ€”the prerequisites are intentionally minimal to make AI engineering accessible to students from diverse backgrounds.",
      "bloom_level": "Remember",
      "difficulty": "easy",
      "concepts": ["Prerequisites"],
      "keywords": ["audience", "prerequisites", "college", "sophomores", "requirements"],
      "source_links": ["docs/course-description.md#prerequisites"],
      "has_example": false,
      "word_count": 47
    },
    {
      "id": "faq-003",
      "category": "Core Concepts",
      "question": "What is Artificial Intelligence?",
      "answer": "Artificial Intelligence (AI) refers to the simulation of human intelligence processes by machines, particularly computer systems. These processes include learning (acquiring information and rules), reasoning (using rules to reach conclusions), and self-correction. Modern AI includes narrow AI (systems designed for specific tasks like facial recognition), machine learning (systems that improve through experience), and deep learning (neural networks with multiple layers). Conversational AI systems are examples of narrow AI specialized for language understanding and generation.",
      "bloom_level": "Understand",
      "difficulty": "easy",
      "concepts": ["Artificial Intelligence", "Machine Learning", "Deep Learning", "Narrow AI"],
      "keywords": ["AI", "artificial intelligence", "machine learning", "definition"],
      "source_links": ["docs/chapters/01-foundations-ai-nlp/index.md#what-is-artificial-intelligence", "docs/glossary.md#artificial-intelligence"],
      "has_example": true,
      "word_count": 79
    },
    {
      "id": "faq-004",
      "category": "Core Concepts",
      "question": "What is a Large Language Model?",
      "answer": "A Large Language Model (LLM) is a neural network with billions of parameters trained on vast text corpora that can understand and generate human-like text for various natural language tasks. Examples include GPT-4, Claude, and PaLM. LLMs use transformer architecture with attention mechanisms to process context and generate coherent, contextually appropriate responses. They're the foundation of modern conversational AI systems.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["Large Language Model", "Transformer Architecture", "Attention Mechanism"],
      "keywords": ["LLM", "language model", "GPT", "Claude", "transformer"],
      "source_links": ["docs/chapters/04-large-language-models-tokenization/index.md", "docs/glossary.md#large-language-model"],
      "has_example": true,
      "word_count": 67
    },
    {
      "id": "faq-005",
      "category": "Core Concepts",
      "question": "What is the RAG pattern?",
      "answer": "RAG (Retrieval Augmented Generation) is an architecture that combines information retrieval with language generation. It works in three steps: (1) Retrieval - finding relevant documents from a knowledge base using semantic search, (2) Augmentation - adding retrieved documents to the user's query to create an enriched prompt, and (3) Generation - using an LLM to generate a response based on the augmented context. RAG enables chatbots to provide accurate, up-to-date information grounded in specific knowledge sources.",
      "bloom_level": "Understand",
      "difficulty": "medium",
      "concepts": ["RAG Pattern", "Retrieval Augmented Generation", "Retrieval Step", "Augmentation Step", "Generation Step"],
      "keywords": ["RAG", "retrieval augmented generation", "architecture", "pattern"],
      "source_links": ["docs/chapters/09-rag-pattern/index.md", "docs/glossary.md#retrieval-augmented-generation"],
      "has_example": true,
      "word_count": 83
    },
    {
      "id": "faq-006",
      "category": "Core Concepts",
      "question": "How does semantic search differ from keyword search?",
      "answer": "Keyword search matches exact words or phrases in queries against indexed content without understanding meaning. Semantic search understands the intent and contextual meaning behind queries using techniques like embeddings and vector similarity. For example, keyword search for 'chatbot tutorial' only finds documents with those exact words, while semantic search would also retrieve 'conversational AI guide' and 'how to build a bot' because they're semantically similar.",
      "bloom_level": "Analyze",
      "difficulty": "medium",
      "concepts": ["Keyword Search", "Semantic Search", "Embedding Vector", "Vector Similarity"],
      "keywords": ["semantic search", "keyword search", "difference", "embeddings", "vector"],
      "source_links": ["docs/chapters/03-semantic-search-quality-metrics/index.md", "docs/glossary.md#semantic-search"],
      "has_example": true,
      "word_count": 76
    },
    {
      "id": "faq-007",
      "category": "Technical Detail Questions",
      "question": "What is Byte Pair Encoding?",
      "answer": "Byte Pair Encoding (BPE) is a tokenization method that iteratively merges frequently occurring character pairs to create subword units, balancing vocabulary size with the ability to represent rare words. For example, 'understanding' might tokenize as ['under', 'stand', 'ing']. BPE handles rare words by breaking them into known subword components rather than marking them as unknown tokens. Most modern LLMs use BPE or similar subword tokenization.",
      "bloom_level": "Understand",
      "difficulty": "hard",
      "concepts": ["Byte Pair Encoding", "Tokenization", "Subword Tokenization"],
      "keywords": ["BPE", "byte pair encoding", "tokenization", "subword"],
      "source_links": ["docs/chapters/04-large-language-models-tokenization/index.md", "docs/glossary.md#byte-pair-encoding"],
      "has_example": true,
      "word_count": 73
    },
    {
      "id": "faq-008",
      "category": "Technical Detail Questions",
      "question": "What is FAISS?",
      "answer": "FAISS (Facebook AI Similarity Search) is an open-source library for efficient similarity search and clustering of dense vectors, optimized for billion-scale datasets. It uses approximate nearest neighbor algorithms to find vectors close to a query vector in high-dimensional space, trading perfect accuracy for significant speed improvements. FAISS can index 10 million document embeddings and retrieve the top results in milliseconds, making it essential for production RAG systems.",
      "bloom_level": "Remember",
      "difficulty": "medium",
      "concepts": ["FAISS", "Approximate Nearest Neighbor", "Vector Index"],
      "keywords": ["FAISS", "vector search", "similarity search", "approximate nearest neighbor"],
      "source_links": ["docs/chapters/05-embeddings-vector-databases/index.md", "docs/glossary.md#faiss"],
      "has_example": true,
      "word_count": 74
    },
    {
      "id": "faq-009",
      "category": "Common Challenge Questions",
      "question": "Why is my chatbot giving inconsistent answers to similar questions?",
      "answer": "Inconsistent answers often stem from poor prompt engineering, lack of context management, or non-deterministic LLM behavior. Solutions include: (1) Use deterministic temperature settings (temperature=0) for consistent outputs, (2) Implement robust prompt templates that provide consistent context, (3) Use RAG to ground responses in specific documents rather than relying on model knowledge, (4) Normalize similar queries to use the same retrieval and generation paths, and (5) Implement answer caching for frequently asked questions.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["Response Generation", "Prompt Engineering", "RAG Pattern", "Temperature"],
      "keywords": ["inconsistent", "answers", "similar questions", "troubleshooting"],
      "source_links": [],
      "has_example": false,
      "word_count": 93
    },
    {
      "id": "faq-010",
      "category": "Common Challenge Questions",
      "question": "How do I reduce hallucination in my chatbot?",
      "answer": "Reduce hallucination through: (1) RAG - ground responses in retrieved documents rather than relying on model knowledge alone, (2) Prompt engineering - explicitly instruct the model to only use provided context and admit when it doesn't know, (3) Citation - require the model to cite sources for factual claims, (4) Confidence scoring - have the model rate its confidence and filter low-confidence responses, (5) Validation - implement post-generation fact-checking against known sources, and (6) Human review - for critical applications, use human-in-the-loop validation.",
      "bloom_level": "Apply",
      "difficulty": "hard",
      "concepts": ["Hallucination", "RAG Pattern", "Factual Accuracy", "Prompt Engineering"],
      "keywords": ["hallucination", "reduce", "accuracy", "grounding"],
      "source_links": ["docs/chapters/09-rag-pattern/index.md#hallucination"],
      "has_example": false,
      "word_count": 88
    },
    {
      "id": "faq-011",
      "category": "Best Practice Questions",
      "question": "When should I use RAG vs fine-tuning an LLM?",
      "answer": "Use RAG when: (1) Knowledge changes frequently (news, prices, inventory), (2) You need citations and traceability, (3) You want to quickly update knowledge without retraining, (4) Budget is limited (fine-tuning costs more), and (5) You need to integrate multiple knowledge sources. Use fine-tuning when: (1) Teaching specific styles or formats, (2) Improving performance on specialized tasks, (3) Adapting to domain-specific language, (4) Knowledge is relatively static, and (5) You need the model to 'know' rather than 'retrieve.' Often, combining both works best - fine-tune for domain adaptation, RAG for knowledge.",
      "bloom_level": "Evaluate",
      "difficulty": "hard",
      "concepts": ["RAG Pattern", "Fine-tuning", "Knowledge Management"],
      "keywords": ["RAG", "fine-tuning", "when to use", "comparison"],
      "source_links": [],
      "has_example": false,
      "word_count": 115
    },
    {
      "id": "faq-012",
      "category": "Best Practice Questions",
      "question": "How should I structure my chatbot's knowledge base?",
      "answer": "Best practices: (1) Organize by topic - create clear categories and hierarchies, (2) Use consistent formatting - standardize how information is presented, (3) Include metadata - add title, category, date, author, relevance scores, (4) Chunk appropriately - balance between too granular (loses context) and too large (poor retrieval precision), (5) Add redundancy - important information should appear in multiple forms and contexts, (6) Version control - track changes to identify when responses degrade, (7) Quality over quantity - 100 high-quality, relevant documents beat 10,000 low-quality ones, and (8) Regular maintenance - review and update content based on usage patterns.",
      "bloom_level": "Apply",
      "difficulty": "medium",
      "concepts": ["Knowledge Management", "Document Corpus", "Metadata Tagging"],
      "keywords": ["knowledge base", "structure", "organization", "best practices"],
      "source_links": [],
      "has_example": false,
      "word_count": 115
    },
    {
      "id": "faq-013",
      "category": "Advanced Topics",
      "question": "How do I build a hybrid search system combining keyword and semantic search?",
      "answer": "Implement hybrid search by: (1) Parallel retrieval - run both keyword search (using inverted indexes, BM25) and semantic search (using vector embeddings) in parallel, (2) Score fusion - combine scores from both approaches using weighted sum, reciprocal rank fusion, or learned fusion models, (3) Result merging - deduplicate and rank merged results, (4) Adaptive weighting - adjust weights based on query type (exact terms vs conceptual queries), (5) Reranking - use a reranker model on merged results to final-rank, and (6) Fallback strategy - if semantic search fails, fall back to keyword search or vice versa. Hybrid search often outperforms either approach alone, especially for diverse query types.",
      "bloom_level": "Create",
      "difficulty": "hard",
      "concepts": ["Keyword Search", "Semantic Search", "Search Ranking", "Inverted Index"],
      "keywords": ["hybrid search", "keyword", "semantic", "combining"],
      "source_links": [],
      "has_example": false,
      "word_count": 121
    },
    {
      "id": "faq-014",
      "category": "Advanced Topics",
      "question": "What's involved in building a production-ready GraphRAG system?",
      "answer": "GraphRAG implementation requires: (1) Knowledge graph construction - extract entities and relationships from documents using NER and relation extraction, (2) Graph database - deploy Neo4j or similar graph database for storage, (3) Ontology design - define entity types and relationship types for your domain, (4) Entity linking - disambiguate and link extracted entities to graph nodes, (5) Query translation - convert natural language queries to graph traversals, (6) Hybrid retrieval - combine graph traversal with vector search of document text, (7) Path ranking - score retrieved paths and information by relevance, (8) Response generation - synthesize information from graph paths and documents, and (9) Graph maintenance - update graph as new information arrives. This is significantly more complex than basic RAG.",
      "bloom_level": "Create",
      "difficulty": "hard",
      "concepts": ["GraphRAG Pattern", "Knowledge Graph", "Graph Database", "Neo4j", "Entity Linking"],
      "keywords": ["GraphRAG", "production", "implementation", "knowledge graph"],
      "source_links": ["docs/chapters/10-knowledge-graphs-graphrag/index.md"],
      "has_example": false,
      "word_count": 125
    }
  ],
  "statistics": {
    "average_answer_length": 82,
    "questions_with_examples": 38,
    "questions_with_links": 54,
    "example_coverage_percent": 45,
    "link_coverage_percent": 64
  },
  "usage_notes": "This JSON file contains structured FAQ data suitable for RAG system integration, chatbot training, and semantic search indexing. Each question includes metadata for categorization, difficulty assessment, concept mapping, keyword extraction, and quality metrics."
}

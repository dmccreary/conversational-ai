{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to our website.</p>"},{"location":"contact/","title":"Contact","text":"<p>Please contact me on LinkedIn</p> <p>Thanks! - Dan</p>"},{"location":"course-description/","title":"Course Description for Conversational AI","text":"<p>Title: Conversational AI Grade Level: College Sophomores</p>"},{"location":"course-description/#prerequisites","title":"Prerequisites","text":"<ul> <li>Students should have a background in basic Python programming, the use of a terminal a shell commands and have the ability to install the VSCode IDE on their computer.</li> <li>Students will be required to use github to share their projects, so they must have a GitHub account.</li> </ul> <p>Note</p> <p>We have deliberately minimized the prerequisites for this course to allow non-computer science majors to take this course.  Students that are not familiar with the use of GitHub and shell commands will need to spend more time the first two weeks of class getting familiar with these tools.</p>"},{"location":"course-description/#course-overview","title":"Course Overview","text":"<p>This course teaches how to use AI technologies to build conversational agents such as chatbots. Our first project will cover building a simple chatbot that answers questions about information in text files. We then build more advanced chatbots that can call services. We briefly cover what traditional search is and some of the challenges of traditional keyword search. We then discuss semantic search and how it provides better results than traditional keyword search. We discuss how we measure search quality and introduce terms such as precision, recall and F-Measures.</p> <p>We then discuss search performance including reverse indexes and Page Rank. We then move into AI topics including LLMs, Tokenization and FAQs analysis. We then introduce the concept of logging chatbot results and feedback. We cover the use of chatbot logs to analyze the most frequent questions with incorrect answers. We discuss chatbot user interface tools.</p> <p>Our first project is building a simple chatbot that does keyword only search. Then we introduce embeddings, vector stores  NLP pipelines and the RAG pattern. We show the limitations of RAG and how to improve results with the GraphRAG pattern. We then show how GraphRAG depends on curated knowledge graphs and how these knowledge graphs can become the central nervous system of an organization. </p> <p>Next, we discuss how ChatBots can be used to call database services. We discuss the process of matching questions to queries and extracting parameters from the questions. Finally we discuss the context of the user, security concerns and role-based access. We wrap up with a final capstone project.</p>"},{"location":"course-description/#topics-covered","title":"Topics Covered","text":"<ol> <li>AI</li> <li>AI Timelines</li> <li>AI Doubling Rate</li> <li>Corporate Nervous Systems</li> <li>NLP</li> <li>Search</li> <li>Grep</li> <li>Keyword Search</li> <li>Synonym Expansion</li> <li>Ontology Enriched Search</li> <li>Metadata Tagging</li> <li>Sematic Search</li> <li>Structured Search</li> <li>Search Performance</li> <li>Keyword Search</li> <li>Reverse Indexes</li> <li>Page Rank</li> <li>Vector Search</li> <li>TF-IDF</li> <li>Precision</li> <li>Recall</li> <li>F-Measures</li> <li>F1 Measure</li> <li>LLM</li> <li>Tokenization</li> <li>FAQ Analysis</li> <li>Intent Modeling</li> <li>ChatBot</li> <li>Feedback</li> <li>The AI Flywheel</li> <li>User Interfaces</li> <li>Feedback Buttons</li> <li>Building a Chatbot</li> <li>External Public Knowledge</li> <li>Internal Private Knowledge</li> <li>Embeddings</li> <li>Vector Store</li> <li>The RAG Pattern</li> <li>Limitations of RAG</li> <li>The GraphRAG Pattern</li> <li>Knowledge Graphs</li> <li>Graph Databases</li> <li>Cypher</li> <li>Entity Extraction</li> <li>Named Entity Extraction</li> <li>NLP Pipelines</li> <li>Executing Queries</li> <li>Describing Queries</li> <li>Query Parameters</li> <li>Matching Questions to Queries</li> <li>Extracting Parameters From Questions</li> <li>Securing Queries</li> <li>User Permissions</li> <li>Role-based Access Control</li> <li>Measuring Response Quality</li> <li>Evaluating Chatbots</li> <li>ChatBot KPIs</li> <li>Chatbot Dashboards</li> <li>Acceptance Rate</li> <li>Measuring Query Frequency</li> <li>Pareto Analysis</li> <li>Chatbot Frameworks</li> <li>JavaScript Libraries</li> <li>Logging Chat Responses</li> <li>Storing PII in Chat Logs</li> <li>Privacy</li> <li>Performance Tradeoffs</li> <li>Tuning Chatbots</li> <li>Team Projects</li> <li>Capstone Projects</li> <li>Chatbot Careers</li> <li>Course Evaluation</li> </ol>"},{"location":"course-description/#topics-not-covered","title":"Topics Not Covered","text":"<ol> <li>Details neural networks work</li> <li>Customizing LLMs</li> <li>Building LLMs from scratch</li> <li>LLM performance</li> <li>SPARQL</li> <li>RDF</li> <li>Triples</li> </ol>"},{"location":"course-description/#learning-objectives","title":"Learning Objectives","text":"<p>Bloom Taxonomy Breakdown</p> <p>After this course, students will be able to:</p>"},{"location":"course-description/#remember","title":"Remember","text":"<ul> <li>Define key terms including LLM, tokenization, embeddings, vector stores, and RAG</li> <li>List the components of a conversational AI system</li> <li>Identify the differences between keyword search and semantic search</li> <li>Recall the metrics used to measure search quality (precision, recall, F-measures)</li> <li>Name common chatbot frameworks and JavaScript libraries</li> <li>Recognize the structure of NLP pipelines</li> </ul>"},{"location":"course-description/#understand","title":"Understand","text":"<ul> <li>Explain how semantic search improves upon traditional keyword search</li> <li>Describe the RAG (Retrieval Augmented Generation) pattern and its components</li> <li>Summarize the limitations of RAG and how GraphRAG addresses them</li> <li>Discuss the role of reverse indexes and Page Rank in search performance</li> <li>Explain how embeddings and vector stores enable semantic search</li> <li>Interpret chatbot KPIs and dashboard metrics</li> <li>Clarify the importance of knowledge graphs as organizational nervous systems</li> </ul>"},{"location":"course-description/#apply","title":"Apply","text":"<ul> <li>Build a simple chatbot using keyword search</li> <li>Implement a RAG-based chatbot using embeddings and vector stores</li> <li>Use NLP pipelines to process and analyze text</li> <li>Apply TF-IDF techniques for text analysis</li> <li>Configure logging for chatbot responses</li> <li>Execute queries with extracted parameters from user questions</li> <li>Implement role-based access control for chatbot queries</li> </ul>"},{"location":"course-description/#analyze","title":"Analyze","text":"<ul> <li>Compare the effectiveness of keyword search versus semantic search</li> <li>Examine chatbot logs to identify frequently asked questions with incorrect answers</li> <li>Perform Pareto analysis on query frequency data</li> <li>Break down the differences between RAG and GraphRAG patterns</li> <li>Differentiate between external public knowledge and internal private knowledge sources</li> <li>Analyze user feedback to improve chatbot performance</li> <li>Investigate privacy concerns related to storing PII in chat logs</li> </ul>"},{"location":"course-description/#evaluate","title":"Evaluate","text":"<ul> <li>Assess chatbot response quality using appropriate metrics</li> <li>Critique the trade-offs between different search approaches</li> <li>Judge the acceptance rate and user satisfaction of chatbot responses</li> <li>Evaluate the security implications of query execution and user permissions</li> <li>Determine which chatbot framework best fits specific use cases</li> <li>Appraise the performance trade-offs in chatbot design decisions</li> <li>Measure and evaluate the effectiveness of intent modeling approaches</li> </ul>"},{"location":"course-description/#create","title":"Create","text":"<ul> <li>Design and develop a complete RAG-based chatbot system</li> <li>Construct a GraphRAG implementation with curated knowledge graphs</li> <li>Generate a chatbot dashboard with relevant KPIs and metrics</li> <li>Develop an entity extraction system for building knowledge graphs</li> <li>Design a query matching system that extracts parameters from natural language questions</li> <li>Produce a comprehensive chatbot evaluation framework</li> <li>Complete a capstone project integrating multiple conversational AI concepts</li> </ul>"},{"location":"feedback/","title":"Feedback on Graph Data Modeling","text":"<p>You are welcome to connect with me on anytime on LinkedIn or submit any issues to GitHub Issue Log.  All pull-requests with fixes to errors or additions are always welcome.</p> <p>If you would like to fill out a short survey and give us ideas on how we can create better tools for intelligent textbooks in the future.</p>"},{"location":"glossary/","title":"Glossary of Terms","text":""},{"location":"glossary/#iso-definition","title":"ISO Definition","text":"<p>A term definition is considered to be consistent with ISO metadata registry guideline 11179 if it meets the following criteria:</p> <ol> <li>Precise</li> <li>Concise</li> <li>Distinct</li> <li>Non-circular</li> <li>Unencumbered with business rules</li> </ol>"},{"location":"glossary/#term","title":"Term","text":"<p>This is the definition of the term.</p>"},{"location":"how-we-built-this-site/","title":"How We Built This Site","text":"<p>This page describes how we built this website and some of  the rationale behind why we made various design choices.</p>"},{"location":"how-we-built-this-site/#python","title":"Python","text":"<p>MicroSims are about how we use generative AI to create animations and simulations.  The language of AI is Python.  So we wanted to create a site that could be easily understood by Python developers.</p>"},{"location":"how-we-built-this-site/#mkdocs-vs-docusaurus","title":"Mkdocs vs. Docusaurus","text":"<p>There are two main tools used by Python developers to write documentation: Mkdocs and Docusaurus.  Mkdocs is easier to use and more popular than Docusaurus. Docusaurus is also optimized for single-page applications. Mkdocs also has an extensive library of themes and plugins. None of us are experts in JavaScript or React. Based on our ChatGPT Analysis of the Tradeoffs we chose mkdocs for this site management.</p>"},{"location":"how-we-built-this-site/#github-and-github-pages","title":"GitHub and GitHub Pages","text":"<p>GitHub is a logical choice to store our  site source code and documentation.  GitHub also has a Custom GitHub Action that does auto-deployment if any files on the site change. We don't currently have this action enabled, but other teams can use this feature if they don't have the ability to do a local build with mkdocs.</p> <p>GitHub also has Issues,  Projects and releases that we can use to manage our bugs and tasks.</p> <p>The best practice for low-cost websites that have public-only content is GitHub Pages. Mkdocs has a command (<code>mkdocs gh-deploy</code>) that does deployment directly to GitHub Pages.  This was an easy choice to make.</p>"},{"location":"how-we-built-this-site/#github-clone","title":"GitHub Clone","text":"<p>If you would like to clone this repository, here are the commands:</p> <pre><code>mkdir projects\ncd projects\ngit clone https://github.com/dmccreary/microsims\n</code></pre>"},{"location":"how-we-built-this-site/#after-changes","title":"After Changes","text":"<p>After you make local changes you must do the following:</p> <pre><code># add the new files to a a local commit transaction\ngit add FILES\n# Execute the a local commit with a message about what and why you are doing the commit\ngit commit -m \"comment\"\n# Update the central GitHub repository\ngit push\n</code></pre>"},{"location":"how-we-built-this-site/#material-theme","title":"Material Theme","text":"<p>We had several options when picking a mkdocs theme:</p> <ol> <li>Mkdocs default</li> <li>Readthedocs</li> <li>Third-Party Themes See Ranking</li> </ol> <p>The Material Theme had 16K stars.  No other theme had over a few hundred. This was also an easy design decision.</p> <p>One key criterial was the social Open Graph tags so that when our users post a link to a simulation, the image of the simulation is included in the link.  Since Material supported this, we used the Material theme. You can see our ChatGPT Design Decision Analysis if you want to check our decision process.</p>"},{"location":"how-we-built-this-site/#enable-edit-icon","title":"Enable Edit Icon","text":"<p>To enable the Edit icon on all pages, you must add the edit_uri and the content.action.edit under the theme features area.</p> <pre><code>edit_uri: edit/master/docs/\n</code></pre> <pre><code>    theme:\n        features:\n            - content.action.edit\n</code></pre>"},{"location":"how-we-built-this-site/#conda-vs-venv","title":"Conda vs VENV","text":"<p>There are two choices for virtual environments.  We can use the native Python venv or use Conda.  venv is simle but is only designed for pure Python projects.  We imagine that this site could use JavaScript and other langauges in the future, so we picked Conda. There is nothing on this microsite that prevents you from using one or the other.  See the ChatGPT Analysis Here.</p> <p>Here is the conda script that we ran to create a new mkdocs environment that also supports the material social imaging libraries.</p> <pre><code>conda deactivate\nconda create -n mkdocs python=3\nconda activate mkdocs\npip install mkdocs \"mkdocs-material[imaging]\"\n</code></pre>"},{"location":"how-we-built-this-site/#mkdocs-commands","title":"Mkdocs Commands","text":"<p>There are three simple mkdoc commands we use.</p>"},{"location":"how-we-built-this-site/#local-build","title":"Local Build","text":"<pre><code>mkdocs build\n</code></pre> <p>This builds your website in a folder called <code>site</code>.  Use this to test that the mkdocs.yml site is working and does not have any errors.</p>"},{"location":"how-we-built-this-site/#run-a-local-server","title":"Run a Local Server","text":"<pre><code>mkdocs serve\n</code></pre> <p>This runs a server on <code>http://localhost:8000</code>. Use this to test the display formatting locally before you push your code up to the GitHub repo.</p> <pre><code>mkdoc gh-deploy\n</code></pre> <p>This pushes everything up to the GitHub Pages site. Note that it does not commit your code to GitHub.</p>"},{"location":"how-we-built-this-site/#mkdocs-material-social-tags","title":"Mkdocs Material Social Tags","text":"<p>We are using the Material Social tags.  This is a work in progress!</p> <p>Here is what we have learned.</p> <ol> <li>There are extensive image processing libraries that can't be installed with just pip.  You will need to run a tool like brew on the Mac to get the libraries installed.</li> <li>Even after <code>brew</code> installs the libraries, you have to get your environment to find the libraries.  The only way I could get that to work was to set up a local UNIX environment variable.</li> </ol> <p>Here is the brew command that I ran:</p> <pre><code>brew install cairo freetype libffi libjpeg libpng zlib\n</code></pre> <p>I then had to add the following to my ~/.zshrc file:</p> <pre><code>export DYLD_FALLBACK_LIBRARY_PATH=/opt/homebrew/lib\n</code></pre> <p>Note that I am running on a Mac with Apple silicon.  This means that the image libraries that brew downloads must be specific to the Mac Arm instruction set.</p>"},{"location":"how-we-built-this-site/#image-generation-and-compression","title":"Image Generation and Compression","text":"<p>I have used ChatGPT to create most of my images.  However, they are too large for most websites.  To compress them down I used  https://tinypng.com/ which is a free tool  for compressing png images without significant loss of quality.  The files created with ChatGPT are typically around 1-2 MB.  After  using the TinyPNG site the size is typically around 200-300KB.</p> <ul> <li>Cover images for blog post #4364</li> <li>Discussion on overriding the Social Card Image</li> </ul>"},{"location":"license/","title":"Creative Commons License","text":"<p>All content in this repository is governed by the following license agreement:</p>"},{"location":"license/#license-type","title":"License Type","text":"<p>Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0 DEED)</p>"},{"location":"license/#link-to-license-agreement","title":"Link to License Agreement","text":"<p>https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en</p>"},{"location":"license/#your-rights","title":"Your Rights","text":"<p>You are free to:</p> <ul> <li>Share \u2014 copy and redistribute the material in any medium or format</li> <li>Adapt \u2014 remix, transform, and build upon the material</li> </ul> <p>The licensor cannot revoke these freedoms as long as you follow the license terms.</p>"},{"location":"license/#restrictions","title":"Restrictions","text":"<ul> <li>Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.</li> <li>NonCommercial \u2014 You may not use the material for commercial purposes.</li> <li>ShareAlike \u2014 If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original.</li> <li>No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.</li> </ul> <p>Notices</p> <p>You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.</p> <p>No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.</p> <p>This deed highlights only some of the key features and terms of the actual license. It is not a license and has no legal value. You should carefully review all of the terms and conditions of the actual license before using the licensed material.</p>"},{"location":"references/","title":"Site References","text":"<ol> <li>mkdocs - https://www.mkdocs.org/ - this is our tool for building the website.  It converts Markdown into HTML in the <code>site</code> directory.</li> <li>mkdocs material theme - https://squidfunk.github.io/mkdocs-material/ - this is the theme for our site.  The theme adds the user interface elements that give our site the look and feel.  It also has the features such as social cards.</li> <li>GitHub Pages - https://pages.github.com/ - this is the free tool for hosting public websites created by mkdocs</li> <li>Markdown - https://www.mkdocs.org/user-guide/writing-your-docs/#writing-with-markdown - this is the format we use for text.  It allows us to have headers, lists, tables, links and images without learning HTML.</li> <li>Deploy Mkdocs GitHub Action - https://github.com/marketplace/actions/deploy-mkdocs - this is the tool we use to automatically build our site after edits are checked in with Git.</li> <li>Git Book - https://git-scm.com/book/en/v2 - a useful book on Git.  Just read the first two chapters to learn how to check in new code.</li> <li>Conda - https://conda.io/ - this is a command line tool that keeps our Python libraries organized for each project.</li> <li>VS Code - https://code.visualstudio.com/ - this is the integrated development environment we use to mange the files on our website.</li> <li>Markdown Paste - https://marketplace.visualstudio.com/items?itemName=telesoho.vscode-markdown-paste-image - this is the VS code extension we use to make sure we keep the markdown format generated by ChatGPT.</li> </ol>"},{"location":"chapters/","title":"Chapters","text":"<p>This textbook is organized into 14 chapters covering 200 concepts in Conversational AI.</p>"},{"location":"chapters/#chapter-overview","title":"Chapter Overview","text":"<ol> <li> <p>Foundations of Artificial Intelligence and Natural Language Processing - This chapter introduces core AI concepts, timelines, and foundational NLP principles including text processing, string matching, and regular expressions.</p> </li> <li> <p>Search Technologies and Indexing Techniques - This chapter covers fundamental search approaches including keyword search, search indexing, inverted indexes, full-text search, and Boolean search operators.</p> </li> <li> <p>Semantic Search and Quality Metrics - This chapter explores advanced search techniques including synonym expansion, ontologies, taxonomies, semantic search, TF-IDF, Page Rank, and introduces search quality metrics like precision, recall, F-measures, and confusion matrices.</p> </li> <li> <p>Large Language Models and Tokenization - This chapter introduces large language models, transformer architecture, attention mechanisms, and various tokenization techniques including byte pair encoding.</p> </li> <li> <p>Embeddings and Vector Databases - This chapter covers word embeddings, embedding vectors, vector space models, embedding models (Word2Vec, GloVe, FastText), sentence embeddings, vector databases, and approximate nearest neighbor search algorithms.</p> </li> <li> <p>Building Chatbots and Intent Recognition - This chapter introduces chatbots, conversational agents, dialog systems, intent recognition and modeling, entity extraction, and FAQ systems.</p> </li> <li> <p>Chatbot Frameworks and User Interfaces - This chapter explores chatbot frameworks (Rasa, Dialogflow, LangChain, LlamaIndex), JavaScript libraries, user interface design, chat interfaces, and session management.</p> </li> <li> <p>User Feedback and Continuous Improvement - This chapter covers user feedback mechanisms, feedback buttons, the AI flywheel, continuous improvement cycles, user context, personalization, and chat history management.</p> </li> <li> <p>The Retrieval Augmented Generation Pattern - This chapter introduces the RAG pattern, external and internal knowledge sources, document corpus management, retrieval steps, augmentation, generation, context windows, prompt engineering, and RAG limitations including hallucination.</p> </li> <li> <p>Knowledge Graphs and GraphRAG - This chapter covers knowledge graphs, graph databases, nodes, edges, triples, RDF, graph query languages (OpenCypher, Cypher), Neo4j, GraphRAG patterns, and corporate nervous systems.</p> </li> <li> <p>NLP Pipelines and Text Processing - This chapter explores NLP pipelines, text preprocessing, normalization, stemming, lemmatization, part-of-speech tagging, dependency parsing, and coreference resolution.</p> </li> <li> <p>Database Queries and Parameter Extraction - This chapter covers database queries, SQL, query parameters, parameter extraction, query templates, parameterized queries, natural language to SQL conversion, and slot filling techniques.</p> </li> <li> <p>Security, Privacy, and User Management - This chapter addresses security, authentication, authorization, role-based access control (RBAC), data privacy, PII, GDPR compliance, data retention, logging systems, and log analysis.</p> </li> <li> <p>Evaluation, Optimization, and Career Development - This chapter covers chatbot evaluation metrics, KPIs, dashboards, acceptance rates, user satisfaction, response accuracy, A/B testing, performance tuning, optimization strategies, team projects, capstone projects, and chatbot career paths.</p> </li> </ol>"},{"location":"chapters/#how-to-use-this-textbook","title":"How to Use This Textbook","text":"<p>Progress through the chapters sequentially, as each chapter builds on concepts from previous chapters. The textbook follows a pedagogical progression from foundational AI concepts through search technologies, language models, embeddings, chatbot development, advanced patterns like RAG and GraphRAG, and finally security and evaluation topics. Dependencies between concepts are carefully respected to ensure a smooth learning experience.</p> <p>Note: Each chapter includes a list of concepts covered. Make sure to complete prerequisites before moving to advanced chapters.</p>"},{"location":"chapters/01-foundations-ai-nlp/","title":"Foundations of Artificial Intelligence and Natural Language Processing","text":""},{"location":"chapters/01-foundations-ai-nlp/#summary","title":"Summary","text":"<p>This chapter introduces the foundational concepts of artificial intelligence and natural language processing that underpin all conversational AI systems. You will learn about the history and evolution of AI, key milestones in AI development, and fundamental NLP techniques for text processing. By the end of this chapter, you will understand core AI principles, the exponential growth of AI capabilities, and basic text manipulation techniques including string matching and regular expressions.</p>"},{"location":"chapters/01-foundations-ai-nlp/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 9 concepts from the learning graph:</p> <ol> <li>Artificial Intelligence</li> <li>AI Timeline</li> <li>AI Doubling Rate</li> <li>Moore's Law</li> <li>Natural Language Processing</li> <li>Text Processing</li> <li>String Matching</li> <li>Regular Expressions</li> <li>Grep Command</li> </ol>"},{"location":"chapters/01-foundations-ai-nlp/#prerequisites","title":"Prerequisites","text":"<p>This chapter assumes only the prerequisites listed in the course description. No prior AI or NLP knowledge is required.</p>"},{"location":"chapters/01-foundations-ai-nlp/#introduction-to-artificial-intelligence","title":"Introduction to Artificial Intelligence","text":"<p>Artificial Intelligence (AI) represents one of the most transformative technological developments of the modern era, fundamentally changing how machines interact with information, make decisions, and communicate with humans. At its core, AI encompasses computational systems that can perform tasks traditionally requiring human intelligence, such as visual perception, speech recognition, decision-making, and language translation. This chapter establishes the foundational knowledge needed to understand conversational AI systems by exploring the historical evolution of AI, the exponential growth in computational capabilities, and the fundamental natural language processing techniques that enable machines to understand and generate human language.</p> <p>The field of AI has progressed from early theoretical foundations in the 1950s to today's sophisticated systems that power virtual assistants, chatbots, and language translation services. Understanding this progression provides crucial context for the conversational AI techniques we'll explore throughout this course. Moreover, grasping the exponential nature of AI advancement helps explain why capabilities that seemed impossible a decade ago are now commonplace in consumer applications.</p>"},{"location":"chapters/01-foundations-ai-nlp/#what-is-artificial-intelligence","title":"What is Artificial Intelligence?","text":"<p>Artificial Intelligence refers to the simulation of human intelligence processes by machines, particularly computer systems. These processes include learning (acquiring information and rules for using it), reasoning (using rules to reach approximate or definite conclusions), and self-correction. Modern AI systems typically fall into several categories:</p> <ul> <li>Narrow AI (Weak AI): Systems designed to perform specific tasks, such as facial recognition, voice assistants, or recommendation algorithms</li> <li>General AI (Strong AI): Hypothetical systems with human-like cognitive abilities across diverse domains (not yet achieved)</li> <li>Machine Learning: AI systems that improve automatically through experience without being explicitly programmed</li> <li>Deep Learning: ML approaches using neural networks with multiple layers to progressively extract higher-level features from raw input</li> </ul> <p>Contemporary conversational AI systems primarily leverage narrow AI techniques, specifically those from natural language processing and machine learning. These systems excel at understanding and generating human language within defined contexts, though they lack the general reasoning capabilities of human intelligence.</p> Evolution of Artificial Intelligence Timeline     Type: timeline      Purpose: Illustrate the major milestones in AI development from its inception to modern conversational AI systems      Time period: 1950-2025      Orientation: Horizontal      Events:     - 1950: Alan Turing publishes \"Computing Machinery and Intelligence,\" proposing the Turing Test     - 1956: Dartmouth Conference coins the term \"Artificial Intelligence\" (John McCarthy, Marvin Minsky, et al.)     - 1957: Perceptron algorithm developed by Frank Rosenblatt (early neural network)     - 1966: ELIZA chatbot created by Joseph Weizenbaum (pattern matching conversation)     - 1969-1979: First AI Winter (reduced funding due to unmet expectations)     - 1980-1987: Expert systems boom (rule-based AI for specialized domains)     - 1987-1993: Second AI Winter (expert systems limitations, hardware constraints)     - 1997: IBM Deep Blue defeats world chess champion Garry Kasparov     - 2006: Geoffrey Hinton revitalizes deep learning with breakthrough in training deep networks     - 2011: IBM Watson wins Jeopardy! using natural language processing     - 2012: AlexNet wins ImageNet competition, sparking deep learning revolution     - 2014: Generative Adversarial Networks (GANs) introduced by Ian Goodfellow     - 2017: Transformer architecture published (\"Attention Is All You Need\" paper)     - 2018: BERT (Bidirectional Encoder Representations from Transformers) released by Google     - 2020: GPT-3 demonstrates few-shot learning with 175 billion parameters     - 2022: ChatGPT launches, bringing conversational AI to mainstream adoption     - 2023: GPT-4 and competing models achieve multimodal capabilities     - 2024-2025: Widespread enterprise adoption of conversational AI and RAG systems      Visual style: Horizontal timeline with alternating above/below placement      Color coding:     - Blue: Foundational research era (1950-1980)     - Red: AI Winter periods (1969-1979, 1987-1993)     - Orange: Expert systems and traditional AI (1980-2000)     - Purple: Modern ML renaissance (2000-2012)     - Green: Deep learning era (2012-2020)     - Gold: Transformer and LLM era (2017-present)      Interactive features:     - Hover over each milestone to see detailed description and impact     - Click to expand with key figures and publications     - Highlight different eras by clicking color-coded legend      Implementation: vis-timeline JavaScript library with custom styling  <p>The timeline above demonstrates several critical patterns in AI development. First, progress has been non-linear, with periods of rapid advancement followed by \"AI winters\" when funding and interest declined due to unmet expectations. Second, breakthrough moments often resulted from novel algorithms combined with increased computational power and available data. The 2012 deep learning revolution, for instance, succeeded because GPU computing made training large neural networks practical, while internet-scale datasets provided training material.</p>"},{"location":"chapters/01-foundations-ai-nlp/#the-exponential-growth-of-ai-capabilities","title":"The Exponential Growth of AI Capabilities","text":"<p>Understanding AI's rapid advancement requires examining two interconnected phenomena: Moore's Law and the AI doubling rate. These concepts explain why AI capabilities that were science fiction in the 1990s are now embedded in everyday consumer devices.</p>"},{"location":"chapters/01-foundations-ai-nlp/#moores-law-and-computing-power","title":"Moore's Law and Computing Power","text":"<p>Moore's Law, named after Intel co-founder Gordon Moore, observes that the number of transistors on integrated circuits doubles approximately every two years, leading to exponential increases in computational power while costs decrease. First articulated in 1965, this trend has held remarkably consistent for over five decades, enabling the progression from room-sized mainframes to smartphones with processing power exceeding 1990s supercomputers.</p> <p>For AI development, Moore's Law has profound implications. Training complex neural networks requires massive computational resources\u2014modern large language models consume millions of GPU-hours during training. The exponential increase in available computing power has made previously infeasible AI approaches practical. Deep learning, which requires training networks with millions or billions of parameters, became viable only when GPU computing could process the necessary calculations in reasonable timeframes.</p> <p>The relationship between computational power and AI capability is captured in the following comparison:</p> Era Representative System Transistor Count AI Capability Example Application 1970s Intel 4004 2,300 Rule-based expert systems Medical diagnosis (MYCIN) 1990s Pentium Pro 5.5 million Statistical ML, decision trees Spam filtering 2000s Intel Core 2 291 million Support vector machines, basic NLP Search engine ranking 2010s Intel Core i7 (Skylake) 1.75 billion Deep learning, CNNs Image recognition 2020s Apple M1 Max 57 billion Transformer models, LLMs Conversational AI, ChatGPT"},{"location":"chapters/01-foundations-ai-nlp/#ai-doubling-rate","title":"AI Doubling Rate","text":"<p>While Moore's Law describes hardware capability growth, the AI doubling rate measures the exponential improvement in AI performance on specific tasks. Research from OpenAI and others demonstrates that AI capabilities have been doubling approximately every 3.4 months in recent years, far exceeding Moore's Law's two-year doubling period. This acceleration results from algorithmic innovations, better training techniques, larger datasets, and architectural improvements, not merely hardware advances.</p> AI Performance Doubling Rate Visualization     Type: chart      Chart type: Line chart with logarithmic Y-axis      Purpose: Show the exponential improvement in AI performance on ImageNet classification task from 2010-2023, demonstrating doubling rate faster than Moore's Law      X-axis: Year (2010-2023)     Y-axis: ImageNet Top-5 Error Rate (%, logarithmic scale from 1% to 50%)      Data series:     1. AI Performance (blue line with markers):        - 2010: 28.2% error (baseline)        - 2011: 25.8% error        - 2012: 16.4% error (AlexNet breakthrough)        - 2013: 11.7% error        - 2014: 7.3% error (GoogLeNet, VGG)        - 2015: 3.6% error (ResNet)        - 2016: 3.0% error        - 2017: 2.3% error (squeeze-and-excitation networks)        - 2018-2023: 1.0-2.0% error (surpassing human performance)      2. Human Performance (horizontal red dashed line):        - Constant at 5.1% error across all years      3. Moore's Law Projected Improvement (orange dotted line):        - Starting at 28.2% in 2010        - Showing theoretical improvement if progress followed hardware doubling (2-year cycle)        - Much slower than actual AI improvement      Title: \"AI Performance Improvement Exceeds Moore's Law\"     Subtitle: \"ImageNet Top-5 Classification Error Rate (2010-2023)\"      Legend: Position top-right      Annotations:     - Arrow at 2012: \"AlexNet: Deep learning breakthrough\"     - Arrow at 2015: \"ResNet: Residual connections enable very deep networks\"     - Horizontal line at human performance: \"Human-level performance (5.1%)\"     - Shaded region below human performance: \"Superhuman performance\"      Key insights callout box:     - \"AI performance doubled every 3.4 months from 2012-2018\"     - \"Exceeded Moore's Law improvement rate by 7x\"     - \"Surpassed human performance in 2015\"      Implementation: Chart.js with logarithmic scale plugin     Canvas size: 800x500px  <p>This acceleration has profound implications for conversational AI. Language understanding capabilities that required extensive manual rule crafting in the 1990s (like ELIZA's pattern matching) now emerge from training large transformer models on internet-scale text corpora. The GPT series exemplifies this trend: GPT-1 (2018) had 117 million parameters, GPT-2 (2019) had 1.5 billion, GPT-3 (2020) had 175 billion, and GPT-4 (2023) is estimated to have over 1 trillion parameters, with each generation demonstrating qualitatively new capabilities.</p>"},{"location":"chapters/01-foundations-ai-nlp/#natural-language-processing-fundamentals","title":"Natural Language Processing Fundamentals","text":"<p>Natural Language Processing (NLP) constitutes the subfield of AI focused on enabling computers to understand, interpret, and generate human language. Unlike programming languages with rigid syntax and unambiguous semantics, natural languages exhibit ambiguity, context-dependence, and cultural variation. NLP systems must handle these complexities while extracting meaningful information from text or speech.</p> <p>Modern conversational AI systems rely heavily on NLP techniques across several stages:</p> <ul> <li>Preprocessing: Cleaning and normalizing text (removing punctuation, converting to lowercase, handling special characters)</li> <li>Tokenization: Breaking text into individual units (words, subwords, or characters)</li> <li>Linguistic Analysis: Understanding grammar, parts of speech, and sentence structure</li> <li>Semantic Understanding: Extracting meaning, intent, and context</li> <li>Generation: Producing grammatically correct and contextually appropriate responses</li> </ul> <p>This course focuses primarily on conversational AI applications, but understanding fundamental text processing techniques provides essential groundwork for the more advanced embedding and transformer-based approaches we'll explore in later chapters.</p>"},{"location":"chapters/01-foundations-ai-nlp/#text-processing-basics","title":"Text Processing Basics","text":"<p>Before applying sophisticated machine learning models, NLP systems typically perform basic text processing to standardize and clean input data. These preprocessing steps ensure consistency and reduce noise that could confuse downstream algorithms.</p> <p>Common text processing operations include:</p> <ol> <li>Case normalization: Converting all text to lowercase to treat \"Python,\" \"python,\" and \"PYTHON\" as identical</li> <li>Whitespace handling: Removing extra spaces, tabs, and newlines</li> <li>Punctuation processing: Either removing or standardizing punctuation marks</li> <li>Number handling: Deciding whether to preserve numeric values or convert them to text</li> <li>Special character removal: Filtering out emoji, symbols, or non-alphanumeric characters depending on application needs</li> </ol> <p>Consider processing user input to a chatbot. The raw input \"Hello!!!   How's your  performance today?\" might be normalized to \"hello how's your performance today\" before further analysis. This standardization ensures that pattern matching and text search operations function reliably.</p> Text Processing Pipeline Workflow     Type: workflow      Purpose: Illustrate the typical stages in preprocessing text for NLP applications      Visual style: Flowchart with process rectangles connected by arrows      Steps:     1. Start: \"Raw Text Input\"        Hover text: \"Example: 'Hello!!! How's your performance TODAY? :)'\"      2. Process: \"Lowercase Conversion\"        Hover text: \"Convert all characters to lowercase for case-insensitive matching\"        Result: \"hello!!! how's your performance today? :)\"      3. Process: \"Special Character Removal\"        Hover text: \"Remove or replace emoji, excessive punctuation, and non-alphanumeric characters\"        Result: \"hello how's your performance today\"      4. Process: \"Whitespace Normalization\"        Hover text: \"Replace multiple spaces with single space, trim leading/trailing whitespace\"        Result: \"hello how's your performance today\"      5. Decision: \"Keep Punctuation?\"        Hover text: \"Application-dependent: keep for sentence splitting, remove for keyword matching\"      6a. Process: \"Remove Punctuation\" (if No)         Hover text: \"Strip all punctuation marks\"         Result: \"hello hows your performance today\"      6b. Process: \"Preserve Punctuation\" (if Yes)         Hover text: \"Maintain punctuation for sentence boundary detection\"         Result: \"hello how's your performance today\"      7. Process: \"Tokenization\"        Hover text: \"Split text into individual tokens (words or subwords)\"        Result: \"['hello', 'how's', 'your', 'performance', 'today']\"      8. Decision: \"Apply Stemming/Lemmatization?\"        Hover text: \"Reduce words to root forms (e.g., 'running' \u2192 'run')\"      9a. Process: \"Apply Morphological Processing\" (if Yes)         Hover text: \"Stemming (simple suffix removal) or lemmatization (dictionary-based root forms)\"      9b. Process: \"Keep Original Tokens\" (if No)         Hover text: \"Preserve original word forms\"      10. End: \"Processed Tokens Ready for Analysis\"         Hover text: \"Clean tokens ready for search, classification, or embedding\"      Color coding:     - Light blue: Input/output     - Green: Text transformation steps     - Yellow: Decision points     - Purple: Final tokenization      Implementation: Mermaid.js flowchart     Canvas size: 800x700px"},{"location":"chapters/01-foundations-ai-nlp/#string-matching-techniques","title":"String Matching Techniques","text":"<p>String matching forms the foundation of text search and pattern recognition. At its simplest, string matching determines whether a specific sequence of characters (the pattern) appears within a larger text (the target). While modern NLP systems employ sophisticated semantic search techniques, understanding basic string matching remains essential for tasks like exact keyword search, code analysis, and log file processing.</p>"},{"location":"chapters/01-foundations-ai-nlp/#exact-matching","title":"Exact Matching","text":"<p>Exact string matching searches for literal character sequences. In Python, this is straightforward using the <code>in</code> operator or string methods:</p> <pre><code>text = \"natural language processing enables conversational ai\"\npattern = \"language processing\"\n\nif pattern in text:\n    print(f\"Found '{pattern}' in text\")\n# Output: Found 'language processing' in text\n</code></pre> <p>Exact matching proves useful for finding specific terms, codes, or identifiers but fails when text variations exist. Searching for \"color\" won't find \"colour,\" and searching for \"AI\" won't match \"artificial intelligence\" unless explicitly programmed to handle synonyms.</p>"},{"location":"chapters/01-foundations-ai-nlp/#case-insensitive-matching","title":"Case-Insensitive Matching","text":"<p>Many search scenarios require case-insensitive matching. This can be achieved by normalizing both the pattern and text to the same case:</p> <pre><code>text = \"Natural Language Processing enables Conversational AI\"\npattern = \"LANGUAGE PROCESSING\"\n\nif pattern.lower() in text.lower():\n    print(\"Match found (case-insensitive)\")\n</code></pre>"},{"location":"chapters/01-foundations-ai-nlp/#substring-search-and-position-finding","title":"Substring Search and Position Finding","text":"<p>Beyond boolean matching (does the pattern exist?), applications often need to locate where patterns occur or extract surrounding context:</p> <pre><code>text = \"NLP includes tokenization, parsing, and semantic analysis\"\npattern = \"parsing\"\n\nposition = text.find(pattern)\nif position != -1:\n    print(f\"Found '{pattern}' at position {position}\")\n    # Extract context: 10 characters before and after\n    start = max(0, position - 10)\n    end = min(len(text), position + len(pattern) + 10)\n    context = text[start:end]\n    print(f\"Context: ...{context}...\")\n</code></pre>"},{"location":"chapters/01-foundations-ai-nlp/#regular-expressions-for-pattern-matching","title":"Regular Expressions for Pattern Matching","text":"<p>While exact string matching handles literal text search, regular expressions (regex) provide a powerful language for describing text patterns. Regular expressions allow matching classes of strings rather than specific strings, enabling flexible pattern recognition essential for many NLP tasks.</p> <p>A regular expression defines a search pattern using ordinary characters (like 'a' or '1') combined with special metacharacters that represent classes or quantities of characters:</p> <p>Common regex metacharacters and patterns:</p> Pattern Meaning Example Matches <code>.</code> Any single character <code>c.t</code> \"cat\", \"cot\", \"c9t\" <code>*</code> Zero or more of preceding <code>ab*c</code> \"ac\", \"abc\", \"abbc\" <code>+</code> One or more of preceding <code>ab+c</code> \"abc\", \"abbc\" (not \"ac\") <code>?</code> Zero or one of preceding <code>colou?r</code> \"color\", \"colour\" <code>\\d</code> Any digit <code>\\d{3}</code> \"123\", \"456\" <code>\\w</code> Any word character (letter, digit, underscore) <code>\\w+</code> \"hello\", \"test_123\" <code>\\s</code> Any whitespace <code>hello\\s+world</code> \"hello world\", \"hello  world\" <code>[abc]</code> Any character in set <code>[Pp]ython</code> \"Python\", \"python\" <code>[a-z]</code> Any character in range <code>[0-9]{2}</code> \"42\", \"99\" <code>^</code> Start of string <code>^Hello</code> \"Hello world\" (not \"Say Hello\") <code>$</code> End of string <code>world$</code> \"Hello world\" (not \"world peace\") <p>Regular expressions excel at tasks like:</p> <ul> <li>Email validation: Ensuring user input matches email format patterns</li> <li>Phone number extraction: Finding phone numbers regardless of formatting (123-456-7890, (123) 456-7890, etc.)</li> <li>URL parsing: Extracting domain names, paths, or parameters from web addresses</li> <li>Date formatting: Recognizing various date representations (2024-01-15, 01/15/2024, January 15, 2024)</li> <li>Log file analysis: Extracting timestamps, error codes, or user IDs from structured logs</li> </ul>"},{"location":"chapters/01-foundations-ai-nlp/#python-regular-expression-examples","title":"Python Regular Expression Examples","text":"<p>Python's <code>re</code> module provides regular expression functionality:</p> <pre><code>import re\n\n# Example 1: Email validation\nemail_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\nemails = [\"user@example.com\", \"invalid.email\", \"test.user+filter@domain.co.uk\"]\n\nfor email in emails:\n    if re.match(email_pattern, email):\n        print(f\"Valid: {email}\")\n    else:\n        print(f\"Invalid: {email}\")\n\n# Example 2: Extract all numbers from text\ntext = \"The model achieved 94.7% accuracy on 1,250 test samples.\"\nnumbers = re.findall(r'\\d+\\.?\\d*', text)\nprint(f\"Numbers found: {numbers}\")  # ['94.7', '1', '250']\n\n# Example 3: Find hashtags in social media text\ntweet = \"Excited about #AI and #MachineLearning! #NLP is fascinating.\"\nhashtags = re.findall(r'#\\w+', tweet)\nprint(f\"Hashtags: {hashtags}\")  # ['#AI', '#MachineLearning', '#NLP']\n\n# Example 4: Replace multiple spaces with single space\nmessy_text = \"Too    many     spaces    here\"\ncleaned = re.sub(r'\\s+', ' ', messy_text)\nprint(f\"Cleaned: {cleaned}\")  # \"Too many spaces here\"\n</code></pre> Interactive Regular Expression Pattern Matcher MicroSim     Type: microsim      Learning objective: Allow students to experiment with regular expression patterns and immediately see what text they match, building intuition for regex syntax and capabilities      Canvas layout (900x700px):     - Top section (900x150): Input area     - Middle section (900x400): Main visualization area     - Right section (200x400): Control panel     - Bottom section (900x150): Results and explanation area      Visual elements:      Top section:     - Text area: \"Enter test text\" (600px wide)     - Text input: \"Enter regex pattern\" (600px wide)     - Example text: \"Contact us at support@example.com or call (555) 123-4567. Visit https://www.example.com for more info.\"      Middle visualization area:     - Display the test text with matches highlighted in yellow     - Show capture groups in different colors (green, blue, purple)     - Display line numbers if multiline text     - Highlight current match when hovering      Right control panel:     - Dropdown: \"Example patterns\" with options:       - Email addresses       - Phone numbers       - URLs       - Dates       - Numbers       - Hashtags       - Custom     - Checkboxes for regex flags:       - Case insensitive (i)       - Multiline (m)       - Global (g)       - Dot matches all (s)     - Button: \"Test Pattern\"     - Button: \"Clear\"     - Display: Match count      Bottom results area:     - List of all matches found     - For each match: show the matched text, position (start-end), and any capture groups     - Explanation panel: dynamically explain what each part of the regex pattern means      Default parameters:     - Pattern: `\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b`     - Test text: \"Contact us at support@example.com or sales@company.org\"     - Flags: Global enabled      Example patterns (selectable from dropdown):     1. Email: `\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b`     2. Phone (US): `\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}`     3. URL: `https?://[^\\s]+`     4. Date (YYYY-MM-DD): `\\d{4}-\\d{2}-\\d{2}`     5. Hashtag: `#\\w+`     6. Numbers: `\\d+\\.?\\d*`      Behavior:     - When user types or selects a pattern, automatically test against text     - Highlight all matches in the visualization area     - Update match count and results list in real-time     - When hovering over a match in the visualization, highlight the corresponding entry in results list     - When selecting an example pattern, load both the pattern and appropriate test text     - Display error message if regex pattern is invalid      Educational features:     - Pattern explanation panel that breaks down the regex:       - `\\b` = word boundary       - `[A-Za-z0-9._%+-]+` = one or more email-valid characters       - `@` = literal @ symbol       - etc.     - Show capture groups with labels if pattern includes groups     - Provide hints for common regex mistakes      Implementation notes:     - Use p5.js for rendering and interaction     - Use JavaScript RegExp for pattern matching     - Store example patterns as array of objects with {name, pattern, testText, explanation}     - Update visualization on each text or pattern change (debounce input for performance)     - Use different highlight colors for different capture groups     - Canvas size: 900x700px      Accessibility:     - Provide text description of matches for screen readers     - Keyboard shortcuts: Ctrl+Enter to test pattern, Esc to clear  <p>The interactive MicroSim above allows experimentation with regex patterns, building intuition for this powerful text processing tool. Regular expressions become particularly important when building conversational AI systems that need to extract structured information from user queries\u2014for instance, parsing dates from \"What's the weather next Friday?\" or extracting product codes from \"Show me details for item SKU-12345.\"</p>"},{"location":"chapters/01-foundations-ai-nlp/#the-grep-command-pattern-search-in-files","title":"The Grep Command: Pattern Search in Files","text":"<p>The <code>grep</code> command (Global Regular Expression Print) represents one of the most essential text processing utilities in Unix/Linux environments. Originally developed in the 1970s, grep searches files or streams for lines matching a pattern and prints those lines to standard output. While seemingly simple, grep's power and flexibility have made it indispensable for developers, system administrators, and data analysts.</p>"},{"location":"chapters/01-foundations-ai-nlp/#basic-grep-usage","title":"Basic Grep Usage","text":"<p>At its core, grep takes a pattern and one or more files, printing lines that match:</p> <pre><code># Search for the word \"error\" in a log file\ngrep \"error\" application.log\n\n# Search case-insensitively\ngrep -i \"error\" application.log  # matches \"Error\", \"ERROR\", \"error\"\n\n# Search recursively in all files within a directory\ngrep -r \"TODO\" ./src/\n\n# Count matching lines instead of displaying them\ngrep -c \"warning\" system.log\n\n# Show line numbers with matches\ngrep -n \"exception\" debug.log\n</code></pre>"},{"location":"chapters/01-foundations-ai-nlp/#grep-with-regular-expressions","title":"Grep with Regular Expressions","text":"<p>Grep supports regular expressions, enabling sophisticated pattern searches:</p> <pre><code># Find lines containing email addresses\ngrep -E '\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b' contacts.txt\n\n# Find lines starting with \"Error:\" followed by a number\ngrep '^Error: [0-9]' logs/*.log\n\n# Find Python function definitions (lines starting with \"def \")\ngrep '^\\s*def\\s' *.py\n\n# Find lines with 3-digit numbers\ngrep '\\b[0-9]{3}\\b' data.txt\n</code></pre>"},{"location":"chapters/01-foundations-ai-nlp/#practical-grep-applications-in-nlp-and-ai-development","title":"Practical Grep Applications in NLP and AI Development","text":"<p>Grep proves invaluable when working with conversational AI systems:</p> <ol> <li>Log analysis: Finding errors, specific user queries, or response patterns in chatbot interaction logs</li> <li>Code search: Locating function definitions, API calls, or configuration parameters across codebases</li> <li>Data exploration: Quickly sampling records from large text datasets before loading into Python</li> <li>Debugging: Finding where specific variables or functions are used during troubleshooting</li> <li>Data validation: Checking if expected patterns appear in output files</li> </ol> <p>Example workflow for analyzing chatbot logs:</p> <pre><code># Find all queries about pricing\ngrep -i \"price\\|cost\\|pricing\" chatbot_logs.txt &gt; pricing_queries.txt\n\n# Count how many times users encountered errors\ngrep -c \"ERROR\" chatbot_logs.txt\n\n# Extract timestamp and error message for all failures\ngrep \"ERROR\" chatbot_logs.txt | grep -oE '[0-9]{4}-[0-9]{2}-[0-9]{2} [0-9]{2}:[0-9]{2}:[0-9]{2}.*'\n\n# Find queries that mentioned specific products\ngrep -E \"(product|item).*[A-Z]{2,4}-[0-9]{4,6}\" chatbot_logs.txt\n</code></pre> <p>Common grep options:</p> Option Purpose Example Usage <code>-i</code> Case-insensitive search <code>grep -i \"python\" file.txt</code> <code>-v</code> Invert match (show non-matching lines) <code>grep -v \"test\" data.txt</code> <code>-r</code> or <code>-R</code> Recursive directory search <code>grep -r \"function\" ./src/</code> <code>-n</code> Show line numbers <code>grep -n \"error\" log.txt</code> <code>-c</code> Count matching lines <code>grep -c \"warning\" log.txt</code> <code>-l</code> Show only filenames with matches <code>grep -l \"TODO\" *.py</code> <code>-A 3</code> Show 3 lines after match <code>grep -A 3 \"exception\" log.txt</code> <code>-B 3</code> Show 3 lines before match <code>grep -B 3 \"error\" log.txt</code> <code>-C 3</code> Show 3 lines of context (before and after) <code>grep -C 3 \"critical\" log.txt</code> <code>-E</code> Extended regex (supports +, ?, |, etc.) <code>grep -E \"error\\|warning\" log.txt</code> <code>-w</code> Match whole words only <code>grep -w \"is\" text.txt</code> <p>While modern conversational AI relies primarily on semantic search using embeddings and vector databases (topics we'll cover in later chapters), grep and pattern matching remain essential for data preprocessing, log analysis, and debugging. Understanding these foundational text processing techniques provides context for appreciating why semantic search represents such a significant advancement.</p>"},{"location":"chapters/01-foundations-ai-nlp/#connecting-foundations-to-conversational-ai","title":"Connecting Foundations to Conversational AI","text":"<p>The concepts introduced in this chapter form the bedrock for understanding modern conversational AI systems. The exponential growth in AI capabilities, driven by both Moore's Law and algorithmic innovations, explains how today's language models achieve performance that would have seemed impossible even a decade ago. The progression from rule-based chatbots like ELIZA (which relied solely on pattern matching) to modern transformer-based systems demonstrates this evolution clearly.</p> <p>Text processing fundamentals\u2014string matching, regular expressions, and pattern search\u2014remain relevant even in the era of large language models:</p> <ul> <li>Preprocessing: Before text enters embedding models or LLMs, it undergoes cleaning and normalization using techniques discussed in this chapter</li> <li>Hybrid systems: Production chatbots often combine semantic search for understanding with regex-based extraction for structured data (dates, product codes, tracking numbers)</li> <li>Debugging and analysis: Developers use grep and pattern matching to analyze chatbot conversation logs, identify problematic queries, and measure system performance</li> <li>Fallback mechanisms: When semantic understanding fails, rule-based pattern matching can provide fallback responses</li> </ul> <p>As we progress through this course, we'll build increasingly sophisticated conversational AI systems. Chapter 2 introduces keyword search and its limitations, motivating the need for semantic understanding. Later chapters explore embeddings, vector stores, the RAG (Retrieval Augmented Generation) pattern, and GraphRAG implementations. Throughout this progression, the foundational concepts from this chapter\u2014understanding AI's exponential growth, recognizing text processing requirements, and applying pattern matching techniques\u2014will prove essential for both conceptual understanding and practical implementation.</p>"},{"location":"chapters/01-foundations-ai-nlp/#key-takeaways","title":"Key Takeaways","text":"<p>Before moving to the next chapter, ensure you understand these core concepts:</p> <ul> <li>Artificial Intelligence encompasses computational systems performing tasks requiring human-like intelligence, with current conversational AI systems using narrow AI techniques focused on language understanding and generation</li> <li>AI development has progressed non-linearly through multiple boom-and-bust cycles, with the modern deep learning era beginning around 2012 and transformer-based language models emerging in 2017</li> <li>Moore's Law describes the doubling of transistor density every two years, providing the computational foundation for modern AI, while the AI doubling rate shows capability improvements occurring even faster (every 3-4 months)</li> <li>Natural Language Processing enables computers to understand and generate human language through preprocessing, tokenization, linguistic analysis, semantic understanding, and generation</li> <li>Text processing fundamentals include case normalization, whitespace handling, punctuation processing, and tokenization as essential preprocessing steps</li> <li>String matching provides exact or case-insensitive literal text search, useful for specific term identification but limited by its inability to handle variations</li> <li>Regular expressions offer a powerful pattern language enabling flexible matching of character classes, quantities, and positions, essential for extracting structured data from text</li> <li>Grep serves as a command-line tool for pattern searching across files, invaluable for log analysis, code search, and data exploration in AI development workflows</li> </ul> <p>These foundations prepare you for exploring keyword search, semantic search, and the conversational AI architectures that build upon these basic text processing capabilities.</p>"},{"location":"chapters/02-search-technologies-indexing/","title":"Search Technologies and Indexing Techniques","text":""},{"location":"chapters/02-search-technologies-indexing/#summary","title":"Summary","text":"<p>This chapter explores fundamental search technologies and indexing techniques that form the backbone of information retrieval systems. You will learn about different types of search approaches, how search indexes are constructed and used, and techniques for expanding search capabilities beyond simple keyword matching. Understanding these concepts is essential for building effective chatbots that can retrieve relevant information from knowledge bases.</p>"},{"location":"chapters/02-search-technologies-indexing/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 14 concepts from the learning graph:</p> <ol> <li>Keyword Search</li> <li>Search Index</li> <li>Inverted Index</li> <li>Reverse Index</li> <li>Full-Text Search</li> <li>Boolean Search</li> <li>Search Query</li> <li>Query Parser</li> <li>Synonym Expansion</li> <li>Thesaurus</li> <li>Ontology</li> <li>Taxonomy</li> <li>Controlled Vocabulary</li> <li>Metadata</li> </ol>"},{"location":"chapters/02-search-technologies-indexing/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Foundations of Artificial Intelligence and Natural Language Processing</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/03-semantic-search-quality-metrics/","title":"Semantic Search and Quality Metrics","text":""},{"location":"chapters/03-semantic-search-quality-metrics/#summary","title":"Summary","text":"<p>This chapter advances your understanding of search by introducing semantic search techniques that go beyond simple keyword matching, along with methods for measuring search quality. You will learn about metadata tagging, vector-based similarity measures, ranking algorithms like Page Rank and TF-IDF, and critical evaluation metrics including precision, recall, and F-measures. These concepts enable you to build more intelligent search systems and objectively assess their performance.</p>"},{"location":"chapters/03-semantic-search-quality-metrics/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 21 concepts from the learning graph:</p> <ol> <li>Metadata Tagging</li> <li>Dublin Core</li> <li>Semantic Search</li> <li>Vector Similarity</li> <li>Cosine Similarity</li> <li>Euclidean Distance</li> <li>Search Ranking</li> <li>Page Rank Algorithm</li> <li>TF-IDF</li> <li>Term Frequency</li> <li>Document Frequency</li> <li>Search Performance</li> <li>Query Optimization</li> <li>Index Performance</li> <li>Search Precision</li> <li>Search Recall</li> <li>F-Measure</li> <li>F1 Score</li> <li>Confusion Matrix</li> <li>True Positive</li> <li>False Positive</li> </ol>"},{"location":"chapters/03-semantic-search-quality-metrics/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Foundations of Artificial Intelligence and Natural Language Processing</li> <li>Chapter 2: Search Technologies and Indexing Techniques</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/04-large-language-models-tokenization/","title":"Large Language Models and Tokenization","text":""},{"location":"chapters/04-large-language-models-tokenization/#summary","title":"Summary","text":"<p>This chapter introduces large language models (LLMs), the powerful AI systems that enable modern conversational agents to understand and generate human-like text. You will learn about transformer architecture, the attention mechanism that makes LLMs effective, and the critical process of tokenization that converts text into units processable by neural networks. These concepts form the foundation for understanding how chatbots generate intelligent responses.</p>"},{"location":"chapters/04-large-language-models-tokenization/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 7 concepts from the learning graph:</p> <ol> <li>Large Language Model</li> <li>Transformer Architecture</li> <li>Attention Mechanism</li> <li>Token</li> <li>Tokenization</li> <li>Subword Tokenization</li> <li>Byte Pair Encoding</li> </ol>"},{"location":"chapters/04-large-language-models-tokenization/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Foundations of Artificial Intelligence and Natural Language Processing</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/05-embeddings-vector-databases/","title":"Embeddings and Vector Databases","text":""},{"location":"chapters/05-embeddings-vector-databases/#summary","title":"Summary","text":"<p>This chapter explores how words and sentences can be represented as numerical vectors in high-dimensional spaces, enabling machines to understand semantic relationships between text. You will learn about various embedding models including Word2Vec, GloVe, and FastText, understand vector space models and dimensionality, and discover how vector databases enable fast similarity searches. These technologies are essential for semantic search and retrieval-augmented generation systems.</p>"},{"location":"chapters/05-embeddings-vector-databases/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 17 concepts from the learning graph:</p> <ol> <li>Word Embedding</li> <li>Embedding Vector</li> <li>Vector Space Model</li> <li>Vector Dimension</li> <li>Embedding Model</li> <li>Word2Vec</li> <li>GloVe</li> <li>FastText</li> <li>Sentence Embedding</li> <li>Contextual Embedding</li> <li>Vector Database</li> <li>Vector Store</li> <li>Vector Index</li> <li>Approximate Nearest Neighbor</li> <li>FAISS</li> <li>Pinecone</li> <li>Weaviate</li> </ol>"},{"location":"chapters/05-embeddings-vector-databases/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Foundations of Artificial Intelligence and Natural Language Processing</li> <li>Chapter 3: Semantic Search and Quality Metrics</li> <li>Chapter 4: Large Language Models and Tokenization</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/06-building-chatbots-intent/","title":"Building Chatbots and Intent Recognition","text":""},{"location":"chapters/06-building-chatbots-intent/#summary","title":"Summary","text":"<p>This chapter introduces the core concepts and techniques for building conversational agents, focusing on understanding user intentions and extracting relevant information from queries. You will learn about chatbot architectures, dialog systems, intent recognition and classification, entity extraction techniques, and how to build FAQ-based systems. These foundational chatbot concepts prepare you to create intelligent conversational interfaces.</p>"},{"location":"chapters/06-building-chatbots-intent/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 15 concepts from the learning graph:</p> <ol> <li>Chatbot</li> <li>Conversational Agent</li> <li>Dialog System</li> <li>Intent Recognition</li> <li>Intent Modeling</li> <li>Intent Classification</li> <li>Entity Extraction</li> <li>Named Entity Recognition</li> <li>Entity Type</li> <li>Entity Linking</li> <li>FAQ</li> <li>FAQ Analysis</li> <li>Question-Answer Pair</li> <li>User Query</li> <li>User Intent</li> </ol>"},{"location":"chapters/06-building-chatbots-intent/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Foundations of Artificial Intelligence and Natural Language Processing</li> <li>Chapter 4: Large Language Models and Tokenization</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/07-chatbot-frameworks-ui/","title":"Chatbot Frameworks and User Interfaces","text":""},{"location":"chapters/07-chatbot-frameworks-ui/#summary","title":"Summary","text":"<p>This chapter explores the practical tools, frameworks, and interface components used to build production-ready chatbots. You will learn about popular chatbot frameworks like Rasa, Dialogflow, LangChain, and LlamaIndex, discover JavaScript libraries for chatbot development, and understand how to design effective chat user interfaces. Additionally, you will explore conversation management including chat history, context preservation, and session handling.</p>"},{"location":"chapters/07-chatbot-frameworks-ui/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 16 concepts from the learning graph:</p> <ol> <li>Chatbot Response</li> <li>Response Generation</li> <li>Response Quality</li> <li>Response Latency</li> <li>Conversation Context</li> <li>Session Management</li> <li>Chatbot Framework</li> <li>Rasa</li> <li>Dialogflow</li> <li>Botpress</li> <li>LangChain</li> <li>LlamaIndex</li> <li>JavaScript Library</li> <li>Node.js</li> <li>React Chatbot</li> <li>Chat Widget</li> </ol>"},{"location":"chapters/07-chatbot-frameworks-ui/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 6: Building Chatbots and Intent Recognition</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/08-user-feedback-improvement/","title":"User Feedback and Continuous Improvement","text":""},{"location":"chapters/08-user-feedback-improvement/#summary","title":"Summary","text":"<p>This chapter focuses on collecting user feedback to continuously improve chatbot performance through iterative learning cycles. You will learn about feedback mechanisms including thumbs up/down buttons, the AI flywheel concept that drives continuous improvement, and techniques for personalizing chatbot responses based on user context, preferences, and history. Understanding these concepts enables you to build chatbots that learn and improve over time.</p>"},{"location":"chapters/08-user-feedback-improvement/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 15 concepts from the learning graph:</p> <ol> <li>User Feedback</li> <li>Feedback Button</li> <li>Thumbs Up/Down</li> <li>Feedback Loop</li> <li>AI Flywheel</li> <li>Continuous Improvement</li> <li>User Interface</li> <li>Chat Interface</li> <li>Message Bubble</li> <li>Chat History</li> <li>User Context</li> <li>User Profile</li> <li>User Preferences</li> <li>User History</li> <li>Personalization</li> </ol>"},{"location":"chapters/08-user-feedback-improvement/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 6: Building Chatbots and Intent Recognition</li> <li>Chapter 7: Chatbot Frameworks and User Interfaces</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/09-rag-pattern/","title":"The Retrieval Augmented Generation Pattern","text":""},{"location":"chapters/09-rag-pattern/#summary","title":"Summary","text":"<p>This chapter introduces the Retrieval Augmented Generation (RAG) pattern, a powerful technique that enhances LLM responses by retrieving relevant information from external knowledge sources. You will learn about the three-step RAG process (retrieval, augmentation, generation), how to work with both public and private knowledge bases, prompt engineering techniques, context windows, and important limitations including hallucination. The RAG pattern is essential for building chatbots that provide accurate, up-to-date information grounded in specific knowledge sources.</p>"},{"location":"chapters/09-rag-pattern/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 18 concepts from the learning graph:</p> <ol> <li>External Knowledge</li> <li>Public Knowledge Base</li> <li>Internal Knowledge</li> <li>Private Documents</li> <li>Document Corpus</li> <li>RAG Pattern</li> <li>Retrieval Augmented Generation</li> <li>Retrieval Step</li> <li>Augmentation Step</li> <li>Generation Step</li> <li>Context Window</li> <li>Prompt Engineering</li> <li>System Prompt</li> <li>User Prompt</li> <li>RAG Limitations</li> <li>Context Length Limit</li> <li>Hallucination</li> <li>Factual Accuracy</li> </ol>"},{"location":"chapters/09-rag-pattern/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 4: Large Language Models and Tokenization</li> <li>Chapter 5: Embeddings and Vector Databases</li> <li>Chapter 6: Building Chatbots and Intent Recognition</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/10-knowledge-graphs-graphrag/","title":"Knowledge Graphs and GraphRAG","text":""},{"location":"chapters/10-knowledge-graphs-graphrag/#summary","title":"Summary","text":"<p>This chapter explores knowledge graphs as structured representations of information and introduces the GraphRAG pattern that combines graph databases with retrieval-augmented generation. You will learn about graph database fundamentals including nodes, edges, and triples, query languages like Cypher and OpenCypher, the RDF standard, and how knowledge graphs can serve as the \"corporate nervous system\" for organizations. The GraphRAG pattern addresses many limitations of traditional RAG by leveraging the rich relationships encoded in knowledge graphs.</p>"},{"location":"chapters/10-knowledge-graphs-graphrag/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 15 concepts from the learning graph:</p> <ol> <li>GraphRAG Pattern</li> <li>Knowledge Graph</li> <li>Graph Database</li> <li>Node</li> <li>Edge</li> <li>Triple</li> <li>Subject-Predicate-Object</li> <li>RDF</li> <li>Graph Query</li> <li>OpenCypher</li> <li>Cypher Query Language</li> <li>Neo4j</li> <li>Corporate Nervous System</li> <li>Organizational Knowledge</li> <li>Knowledge Management</li> </ol>"},{"location":"chapters/10-knowledge-graphs-graphrag/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 2: Search Technologies and Indexing Techniques</li> <li>Chapter 9: The Retrieval Augmented Generation Pattern</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/11-nlp-pipelines-processing/","title":"NLP Pipelines and Text Processing","text":""},{"location":"chapters/11-nlp-pipelines-processing/#summary","title":"Summary","text":"<p>This chapter covers NLP pipelines and advanced text processing techniques that prepare raw text for analysis and understanding by conversational AI systems. You will learn about text preprocessing steps including normalization, stemming, and lemmatization, as well as linguistic analysis techniques like part-of-speech tagging, dependency parsing, and coreference resolution. These NLP pipeline components are essential for extracting structured information from unstructured text.</p>"},{"location":"chapters/11-nlp-pipelines-processing/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 8 concepts from the learning graph:</p> <ol> <li>NLP Pipeline</li> <li>Text Preprocessing</li> <li>Text Normalization</li> <li>Stemming</li> <li>Lemmatization</li> <li>Part-of-Speech Tagging</li> <li>Dependency Parsing</li> <li>Coreference Resolution</li> </ol>"},{"location":"chapters/11-nlp-pipelines-processing/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 1: Foundations of Artificial Intelligence and Natural Language Processing</li> <li>Chapter 6: Building Chatbots and Intent Recognition</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/12-database-queries-parameters/","title":"Database Queries and Parameter Extraction","text":""},{"location":"chapters/12-database-queries-parameters/#summary","title":"Summary","text":"<p>This chapter teaches how to enable chatbots to execute database queries based on natural language questions, a critical capability for data-driven conversational applications. You will learn about database query fundamentals, SQL query construction, parameter extraction from user questions, query templates and parameterization, natural language to SQL conversion, and slot filling techniques. These skills enable chatbots to answer questions that require accessing structured data from databases.</p>"},{"location":"chapters/12-database-queries-parameters/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 11 concepts from the learning graph:</p> <ol> <li>Database Query</li> <li>SQL Query</li> <li>Query Parameter</li> <li>Parameter Extraction</li> <li>Query Template</li> <li>Parameterized Query</li> <li>Query Execution</li> <li>Query Description</li> <li>Natural Language to SQL</li> <li>Question to Query Mapping</li> <li>Slot Filling</li> </ol>"},{"location":"chapters/12-database-queries-parameters/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 6: Building Chatbots and Intent Recognition</li> <li>Chapter 11: NLP Pipelines and Text Processing</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/13-security-privacy-users/","title":"Security, Privacy, and User Management","text":""},{"location":"chapters/13-security-privacy-users/#summary","title":"Summary","text":"<p>This chapter addresses critical security, privacy, and access control considerations for production chatbot systems. You will learn about authentication and authorization mechanisms, role-based access control (RBAC), data privacy regulations including GDPR, handling personally identifiable information (PII), data retention policies, and logging systems for monitoring and compliance. Understanding these concepts is essential for building chatbots that protect user data and comply with regulatory requirements.</p>"},{"location":"chapters/13-security-privacy-users/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 16 concepts from the learning graph:</p> <ol> <li>Security</li> <li>Authentication</li> <li>Authorization</li> <li>User Permission</li> <li>Role-Based Access Control</li> <li>RBAC</li> <li>Access Policy</li> <li>Data Privacy</li> <li>PII</li> <li>Personally Identifiable Info</li> <li>GDPR</li> <li>Data Retention</li> <li>Log Storage</li> <li>Chat Log</li> <li>Logging System</li> <li>Log Analysis</li> </ol>"},{"location":"chapters/13-security-privacy-users/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 6: Building Chatbots and Intent Recognition</li> <li>Chapter 8: User Feedback and Continuous Improvement</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"chapters/14-evaluation-optimization-careers/","title":"Evaluation, Optimization, and Career Development","text":""},{"location":"chapters/14-evaluation-optimization-careers/#summary","title":"Summary","text":"<p>This chapter covers the evaluation and optimization of chatbot systems, along with career opportunities in the conversational AI field. You will learn about chatbot metrics and KPIs, dashboard design for monitoring performance, techniques for measuring user satisfaction and acceptance rates, A/B testing methodologies, performance tuning strategies, and approaches for team and capstone projects. The chapter concludes with an exploration of career paths in chatbot development and conversational AI.</p>"},{"location":"chapters/14-evaluation-optimization-careers/#concepts-covered","title":"Concepts Covered","text":"<p>This chapter covers the following 18 concepts from the learning graph:</p> <ol> <li>Query Frequency</li> <li>Frequency Analysis</li> <li>Pareto Analysis</li> <li>80/20 Rule</li> <li>Chatbot Metrics</li> <li>KPI</li> <li>Key Performance Indicator</li> <li>Chatbot Dashboard</li> <li>Acceptance Rate</li> <li>User Satisfaction</li> <li>Response Accuracy</li> <li>Chatbot Evaluation</li> <li>A/B Testing</li> <li>Performance Tuning</li> <li>Optimization</li> <li>Team Project</li> <li>Capstone Project</li> <li>Chatbot Career</li> </ol>"},{"location":"chapters/14-evaluation-optimization-careers/#prerequisites","title":"Prerequisites","text":"<p>This chapter builds on concepts from:</p> <ul> <li>Chapter 3: Semantic Search and Quality Metrics</li> <li>Chapter 7: Chatbot Frameworks and User Interfaces</li> <li>Chapter 8: User Feedback and Continuous Improvement</li> <li>Chapter 13: Security, Privacy, and User Management</li> </ul> <p>TODO: Generate Chapter Content</p>"},{"location":"learning-graph/","title":"Learning Graph for Conversational AI","text":"<p>This section contains the learning graph for this textbook. A learning graph is a graph of concepts used in this textbook. Each concept is represented by a node in a network graph. Concepts are connected by directed edges that indicate what concepts each node depends on before that concept is understood by the student.</p> <p>A learning graph is the foundational data structure for intelligent textbooks that can recommend learning paths. A learning graph is like a roadmap of concepts to help students arrive at their learning goals.</p> <p>At the left of the learning graph are prerequisite or foundational concepts. They have no outbound edges. They only have inbound edges for other concepts that depend on understanding these foundational prerequisite concepts. At the far right we have the most advanced concepts in the course. To master these concepts you must understand all the concepts that they point to.</p> <p>Here are other files used by the learning graph.</p>"},{"location":"learning-graph/#course-description","title":"Course Description","text":"<p>We use the Course Description as the source document for the concepts that are included in this course. The course description uses the 2001 Bloom taxonomy to order learning objectives.</p>"},{"location":"learning-graph/#list-of-concepts","title":"List of Concepts","text":"<p>We use generative AI to convert the course description into a Concept List. Each concept is in the form of a short Title Case label with most labels under 32 characters long.</p>"},{"location":"learning-graph/#concept-dependency-list","title":"Concept Dependency List","text":"<p>We next use generative AI to create a Directed Acyclic Graph (DAG). DAGs do not have cycles where concepts depend on themselves. We provide the DAG in two formats. One is a CSV file and the other format is a JSON file that uses the vis-network JavaScript library format. The vis-network format uses <code>nodes</code>, <code>edges</code> and <code>metadata</code> elements with edges containing <code>from</code> and <code>to</code> properties. This makes it easy for you to view and edit the learning graph using an editor built with the vis-network tools.</p>"},{"location":"learning-graph/#analysis-documentation","title":"Analysis &amp; Documentation","text":""},{"location":"learning-graph/#course-description-quality-assessment","title":"Course Description Quality Assessment","text":"<p>This report rates the overall quality of the course description for the purpose of generating a learning graph.</p> <ul> <li>Course description fields and content depth analysis</li> <li>Validates course description has sufficient depth for generating 200 concepts</li> <li>Compares course description against similar courses</li> <li>Identifies content gaps and strengths</li> <li>Suggests areas of improvement</li> </ul> <p>View the Course Description Quality Assessment</p>"},{"location":"learning-graph/#learning-graph-quality-validation","title":"Learning Graph Quality Validation","text":"<p>This report gives you an overall assessment of the quality of the learning graph. It uses graph algorithms to look for specific quality patterns in the graph.</p> <ul> <li>Graph structure validation - all concepts are connected</li> <li>DAG validation (no cycles detected)</li> <li>Foundational concepts: 7 entry points</li> <li>Indegree distribution analysis</li> <li>Longest dependency chains</li> <li>Connectivity: all nodes connected in single graph</li> </ul> <p>View the Learning Graph Quality Validation</p>"},{"location":"learning-graph/#concept-taxonomy","title":"Concept Taxonomy","text":"<p>In order to see patterns in the learning graph, it is useful to assign colors to each concept based on the concept type. We use generative AI to create about a dozen categories for our concepts and then place each concept into a single primary classifier.</p> <ul> <li>A concept classifier taxonomy with 13 categories</li> <li>Category organization - foundational elements first, course projects last</li> <li>Balanced categories (1.5% - 23% each)</li> <li>All categories under 30% threshold</li> <li>Pedagogical flow recommendations</li> <li>Clear 3-5 letter abbreviations for use in CSV file</li> </ul> <p>View the Concept Taxonomy</p>"},{"location":"learning-graph/#taxonomy-distribution","title":"Taxonomy Distribution","text":"<p>This report shows how many concepts fit into each category of the taxonomy. Our goal is a somewhat balanced taxonomy where each category holds an equal number of concepts. We also don't want any category to contain over 30% of our concepts.</p> <ul> <li>Statistical breakdown</li> <li>Detailed concept listing by category</li> <li>Visual distribution table</li> <li>Balance verification</li> </ul> <p>View the Taxonomy Distribution Report</p>"},{"location":"learning-graph/concept-list/","title":"Concept List for Conversational AI Course","text":"<p>This list contains 200 concepts organized to support the learning graph generation.</p> <ol> <li>Artificial Intelligence</li> <li>AI Timeline</li> <li>AI Doubling Rate</li> <li>Moore's Law</li> <li>Natural Language Processing</li> <li>Text Processing</li> <li>String Matching</li> <li>Regular Expressions</li> <li>Grep Command</li> <li>Keyword Search</li> <li>Search Index</li> <li>Inverted Index</li> <li>Reverse Index</li> <li>Full-Text Search</li> <li>Boolean Search</li> <li>Search Query</li> <li>Query Parser</li> <li>Synonym Expansion</li> <li>Thesaurus</li> <li>Ontology</li> <li>Taxonomy</li> <li>Controlled Vocabulary</li> <li>Metadata</li> <li>Metadata Tagging</li> <li>Dublin Core</li> <li>Semantic Search</li> <li>Vector Similarity</li> <li>Cosine Similarity</li> <li>Euclidean Distance</li> <li>Search Ranking</li> <li>Page Rank Algorithm</li> <li>TF-IDF</li> <li>Term Frequency</li> <li>Document Frequency</li> <li>Search Precision</li> <li>Search Recall</li> <li>F-Measure</li> <li>F1 Score</li> <li>Confusion Matrix</li> <li>True Positive</li> <li>False Positive</li> <li>Search Performance</li> <li>Query Optimization</li> <li>Index Performance</li> <li>Large Language Model</li> <li>Transformer Architecture</li> <li>Attention Mechanism</li> <li>Token</li> <li>Tokenization</li> <li>Subword Tokenization</li> <li>Byte Pair Encoding</li> <li>Word Embedding</li> <li>Embedding Vector</li> <li>Vector Space Model</li> <li>Vector Dimension</li> <li>Embedding Model</li> <li>Word2Vec</li> <li>GloVe</li> <li>FastText</li> <li>Sentence Embedding</li> <li>Contextual Embedding</li> <li>Vector Database</li> <li>Vector Store</li> <li>Vector Index</li> <li>Approximate Nearest Neighbor</li> <li>FAISS</li> <li>Pinecone</li> <li>Weaviate</li> <li>Chatbot</li> <li>Conversational Agent</li> <li>Dialog System</li> <li>Intent Recognition</li> <li>Intent Modeling</li> <li>Intent Classification</li> <li>Entity Extraction</li> <li>Named Entity Recognition</li> <li>Entity Type</li> <li>Entity Linking</li> <li>FAQ</li> <li>FAQ Analysis</li> <li>Question-Answer Pair</li> <li>User Query</li> <li>User Intent</li> <li>Chatbot Response</li> <li>Response Generation</li> <li>Response Quality</li> <li>Response Latency</li> <li>User Feedback</li> <li>Feedback Button</li> <li>Thumbs Up/Down</li> <li>Feedback Loop</li> <li>AI Flywheel</li> <li>Continuous Improvement</li> <li>User Interface</li> <li>Chat Interface</li> <li>Message Bubble</li> <li>Chat History</li> <li>Conversation Context</li> <li>Session Management</li> <li>Chatbot Framework</li> <li>Rasa</li> <li>Dialogflow</li> <li>Botpress</li> <li>LangChain</li> <li>LlamaIndex</li> <li>JavaScript Library</li> <li>Node.js</li> <li>React Chatbot</li> <li>Chat Widget</li> <li>External Knowledge</li> <li>Public Knowledge Base</li> <li>Internal Knowledge</li> <li>Private Documents</li> <li>Document Corpus</li> <li>RAG Pattern</li> <li>Retrieval Augmented Generation</li> <li>Retrieval Step</li> <li>Augmentation Step</li> <li>Generation Step</li> <li>Context Window</li> <li>Prompt Engineering</li> <li>System Prompt</li> <li>User Prompt</li> <li>RAG Limitations</li> <li>Context Length Limit</li> <li>Hallucination</li> <li>Factual Accuracy</li> <li>GraphRAG Pattern</li> <li>Knowledge Graph</li> <li>Graph Database</li> <li>Node</li> <li>Edge</li> <li>Triple</li> <li>Subject-Predicate-Object</li> <li>RDF</li> <li>Graph Query</li> <li>OpenCypher</li> <li>Cypher Query Language</li> <li>Neo4j</li> <li>Corporate Nervous System</li> <li>Organizational Knowledge</li> <li>Knowledge Management</li> <li>NLP Pipeline</li> <li>Text Preprocessing</li> <li>Text Normalization</li> <li>Stemming</li> <li>Lemmatization</li> <li>Part-of-Speech Tagging</li> <li>Dependency Parsing</li> <li>Coreference Resolution</li> <li>Database Query</li> <li>SQL Query</li> <li>Query Parameter</li> <li>Parameter Extraction</li> <li>Query Template</li> <li>Parameterized Query</li> <li>Query Execution</li> <li>Query Description</li> <li>Natural Language to SQL</li> <li>Question to Query Mapping</li> <li>Slot Filling</li> <li>User Context</li> <li>User Profile</li> <li>User Preferences</li> <li>User History</li> <li>Personalization</li> <li>Security</li> <li>Authentication</li> <li>Authorization</li> <li>User Permission</li> <li>Role-Based Access Control</li> <li>RBAC</li> <li>Access Policy</li> <li>Data Privacy</li> <li>PII</li> <li>Personally Identifiable Info</li> <li>GDPR</li> <li>Data Retention</li> <li>Log Storage</li> <li>Chat Log</li> <li>Logging System</li> <li>Log Analysis</li> <li>Query Frequency</li> <li>Frequency Analysis</li> <li>Pareto Analysis</li> <li>80/20 Rule</li> <li>Chatbot Metrics</li> <li>KPI</li> <li>Key Performance Indicator</li> <li>Chatbot Dashboard</li> <li>Acceptance Rate</li> <li>User Satisfaction</li> <li>Response Accuracy</li> <li>Chatbot Evaluation</li> <li>A/B Testing</li> <li>Performance Tuning</li> <li>Optimization</li> <li>Team Project</li> <li>Capstone Project</li> <li>Chatbot Career</li> </ol>"},{"location":"learning-graph/concept-taxonomy/","title":"Concept Taxonomy","text":"<p>This taxonomy organizes the 200 concepts into 12 categories for better navigation and understanding.</p>"},{"location":"learning-graph/concept-taxonomy/#1-foundation-concepts-found","title":"1. Foundation Concepts (FOUND)","text":"<p>TaxonomyID: FOUND</p> <p>Description: Core AI and NLP fundamentals that form the basis for conversational AI systems, including basic AI concepts, timelines, and natural language processing principles.</p>"},{"location":"learning-graph/concept-taxonomy/#2-search-technologies-search","title":"2. Search Technologies (SEARCH)","text":"<p>TaxonomyID: SEARCH</p> <p>Description: Various search approaches and algorithms including keyword search, semantic search, full-text search, and search indexing techniques like inverted indexes and Page Rank.</p>"},{"location":"learning-graph/concept-taxonomy/#3-search-quality-metrics-metric","title":"3. Search Quality Metrics (METRIC)","text":"<p>TaxonomyID: METRIC</p> <p>Description: Metrics and measurements for evaluating search quality including precision, recall, F-measures, confusion matrices, and performance indicators.</p>"},{"location":"learning-graph/concept-taxonomy/#4-language-models-llm","title":"4. Language Models (LLM)","text":"<p>TaxonomyID: LLM</p> <p>Description: Large language models, transformer architectures, attention mechanisms, and tokenization techniques including subword tokenization and byte pair encoding.</p>"},{"location":"learning-graph/concept-taxonomy/#5-embeddings-and-vectors-embed","title":"5. Embeddings and Vectors (EMBED)","text":"<p>TaxonomyID: EMBED</p> <p>Description: Word embeddings, sentence embeddings, vector spaces, vector databases, and similarity measures like cosine similarity and Euclidean distance.</p>"},{"location":"learning-graph/concept-taxonomy/#6-chatbot-systems-chat","title":"6. Chatbot Systems (CHAT)","text":"<p>TaxonomyID: CHAT</p> <p>Description: Chatbot fundamentals, conversational agents, dialog systems, intent recognition, FAQ systems, user interfaces, and chatbot frameworks.</p>"},{"location":"learning-graph/concept-taxonomy/#7-rag-patterns-rag","title":"7. RAG Patterns (RAG)","text":"<p>TaxonomyID: RAG</p> <p>Description: Retrieval Augmented Generation patterns, including retrieval steps, augmentation, generation, context windows, prompt engineering, and RAG limitations.</p>"},{"location":"learning-graph/concept-taxonomy/#8-knowledge-graphs-graph","title":"8. Knowledge Graphs (GRAPH)","text":"<p>TaxonomyID: GRAPH</p> <p>Description: Knowledge graphs, graph databases, nodes, edges, triples, RDF, graph query languages (OpenCypher, Cypher), and GraphRAG patterns.</p>"},{"location":"learning-graph/concept-taxonomy/#9-nlp-processing-nlp","title":"9. NLP Processing (NLP)","text":"<p>TaxonomyID: NLP</p> <p>Description: NLP pipelines, text preprocessing, normalization, stemming, lemmatization, part-of-speech tagging, dependency parsing, and entity extraction.</p>"},{"location":"learning-graph/concept-taxonomy/#10-query-systems-query","title":"10. Query Systems (QUERY)","text":"<p>TaxonomyID: QUERY</p> <p>Description: Database queries, SQL, query parameters, parameter extraction, natural language to SQL conversion, and query execution systems.</p>"},{"location":"learning-graph/concept-taxonomy/#11-security-and-privacy-sec","title":"11. Security and Privacy (SEC)","text":"<p>TaxonomyID: SEC</p> <p>Description: Security, authentication, authorization, role-based access control, data privacy, PII, GDPR compliance, logging, and data retention policies.</p>"},{"location":"learning-graph/concept-taxonomy/#12-evaluation-and-optimization-eval","title":"12. Evaluation and Optimization (EVAL)","text":"<p>TaxonomyID: EVAL</p> <p>Description: Chatbot evaluation, KPIs, dashboards, acceptance rates, user satisfaction, feedback systems, A/B testing, performance tuning, and optimization strategies.</p>"},{"location":"learning-graph/concept-taxonomy/#13-tools-and-projects-tool","title":"13. Tools and Projects (TOOL)","text":"<p>TaxonomyID: TOOL</p> <p>Description: Chatbot frameworks (Rasa, Dialogflow, LangChain), JavaScript libraries, development tools, team projects, capstone projects, and career paths.</p>"},{"location":"learning-graph/course-description-assessment/","title":"Course Description Quality Assessment","text":"<p>Overall Score: 95/100</p> <p>Quality Rating: Excellent - Ready for learning graph generation</p>"},{"location":"learning-graph/course-description-assessment/#detailed-scoring-breakdown","title":"Detailed Scoring Breakdown","text":"Element Points Earned Max Points Status Title 5 5 \u2713 Complete Target Audience 5 5 \u2713 Complete Prerequisites 0 5 \u2717 Missing Main Topics Covered 10 10 \u2713 Complete Topics Excluded 5 5 \u2713 Complete Learning Outcomes Header 5 5 \u2713 Complete Remember Level 10 10 \u2713 Complete Understand Level 10 10 \u2713 Complete Apply Level 10 10 \u2713 Complete Analyze Level 10 10 \u2713 Complete Evaluate Level 10 10 \u2713 Complete Create Level 10 10 \u2713 Complete Descriptive Context 5 5 \u2713 Complete"},{"location":"learning-graph/course-description-assessment/#summary","title":"Summary","text":"<p>The course description is excellent and well-prepared for learning graph generation. Key strengths include:</p> <ol> <li>Comprehensive Topic Coverage: 70+ topics spanning AI fundamentals through advanced GraphRAG implementations</li> <li>Excellent Bloom's Taxonomy Coverage: All six cognitive levels have 6-7 well-crafted outcomes each</li> <li>Clear Progression: Logical flow from basic keyword search to advanced GraphRAG patterns</li> <li>Practical Focus: Strong emphasis on hands-on projects</li> <li>Well-Defined Boundaries: Clear \"Topics Not Covered\" section</li> </ol>"},{"location":"learning-graph/course-description-assessment/#estimated-concept-potential","title":"Estimated Concept Potential","text":"<p>220-250 concepts can be derived from this course description, well exceeding the target of 200 concepts.</p>"},{"location":"learning-graph/course-description-assessment/#recommendation","title":"Recommendation","text":"<p>\u2713 Proceed with learning graph generation</p> <p>The quality score of 95/100 indicates this course description is ready for comprehensive learning graph generation.</p>"},{"location":"learning-graph/quality-metrics/","title":"Learning Graph Quality Metrics Report","text":""},{"location":"learning-graph/quality-metrics/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 200</li> <li>Foundational Concepts (no dependencies): 7</li> <li>Concepts with Dependencies: 193</li> <li>Average Dependencies per Concept: 1.24</li> </ul>"},{"location":"learning-graph/quality-metrics/#graph-structure-validation","title":"Graph Structure Validation","text":"<ul> <li>Valid DAG Structure: \u274c No</li> <li>Self-Dependencies: None detected \u2705</li> <li>Cycles Detected: 0</li> </ul>"},{"location":"learning-graph/quality-metrics/#foundational-concepts","title":"Foundational Concepts","text":"<p>These concepts have no prerequisites:</p> <ul> <li>1: Artificial Intelligence</li> <li>54: Vector Space Model</li> <li>94: User Interface</li> <li>106: JavaScript Library</li> <li>110: External Knowledge</li> <li>129: Knowledge Graph</li> <li>151: Database Query</li> </ul>"},{"location":"learning-graph/quality-metrics/#dependency-chain-analysis","title":"Dependency Chain Analysis","text":"<ul> <li>Maximum Dependency Chain Length: 13</li> </ul>"},{"location":"learning-graph/quality-metrics/#longest-learning-path","title":"Longest Learning Path:","text":"<ol> <li>Artificial Intelligence (ID: 1)</li> <li>Natural Language Processing (ID: 5)</li> <li>Chatbot (ID: 69)</li> <li>Security (ID: 167)</li> <li>Data Privacy (ID: 174)</li> <li>Data Retention (ID: 178)</li> <li>Log Storage (ID: 179)</li> <li>Chat Log (ID: 180)</li> <li>Log Analysis (ID: 182)</li> <li>Query Frequency (ID: 183)</li> <li>Frequency Analysis (ID: 184)</li> <li>Pareto Analysis (ID: 185)</li> <li>80/20 Rule (ID: 186)</li> </ol>"},{"location":"learning-graph/quality-metrics/#orphaned-nodes-analysis","title":"Orphaned Nodes Analysis","text":"<ul> <li>Total Orphaned Nodes: 93</li> </ul> <p>Concepts that are not prerequisites for any other concept:</p> <ul> <li>4: Moore's Law</li> <li>9: Grep Command</li> <li>13: Reverse Index</li> <li>14: Full-Text Search</li> <li>15: Boolean Search</li> <li>17: Query Parser</li> <li>19: Thesaurus</li> <li>22: Controlled Vocabulary</li> <li>25: Dublin Core</li> <li>26: Semantic Search</li> <li>28: Cosine Similarity</li> <li>29: Euclidean Distance</li> <li>31: Page Rank Algorithm</li> <li>32: TF-IDF</li> <li>38: F1 Score</li> <li>40: True Positive</li> <li>41: False Positive</li> <li>43: Query Optimization</li> <li>44: Index Performance</li> <li>47: Attention Mechanism</li> </ul> <p>...and 73 more</p>"},{"location":"learning-graph/quality-metrics/#connected-components","title":"Connected Components","text":"<ul> <li>Number of Connected Components: 1</li> </ul> <p>\u2705 All concepts are connected in a single graph.</p>"},{"location":"learning-graph/quality-metrics/#indegree-analysis","title":"Indegree Analysis","text":"<p>Top 10 concepts that are prerequisites for the most other concepts:</p> Rank Concept ID Concept Label Indegree 1 10 Keyword Search 11 2 69 Chatbot 11 3 5 Natural Language Processing 9 4 45 Large Language Model 7 5 129 Knowledge Graph 7 6 187 Chatbot Metrics 7 7 1 Artificial Intelligence 6 8 6 Text Processing 6 9 63 Vector Store 5 10 100 Chatbot Framework 5"},{"location":"learning-graph/quality-metrics/#outdegree-distribution","title":"Outdegree Distribution","text":"Dependencies Number of Concepts 0 7 1 149 2 42 3 2"},{"location":"learning-graph/quality-metrics/#recommendations","title":"Recommendations","text":"<ul> <li>\u26a0\ufe0f Many orphaned nodes (93): Consider if these should be prerequisites for advanced concepts</li> <li>\u2139\ufe0f Consider adding cross-dependencies: More connections could create richer learning pathways</li> </ul> <p>Report generated by learning-graph-reports/analyze_graph.py</p>"},{"location":"learning-graph/taxonomy-distribution/","title":"Taxonomy Distribution Report","text":""},{"location":"learning-graph/taxonomy-distribution/#overview","title":"Overview","text":"<ul> <li>Total Concepts: 200</li> <li>Number of Taxonomies: 13</li> <li>Average Concepts per Taxonomy: 15.4</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#distribution-summary","title":"Distribution Summary","text":"Category TaxonomyID Count Percentage Status CHAT CHAT 46 23.0% \u2705 SEARCH SEARCH 28 14.0% \u2705 RAG RAG 18 9.0% \u2705 EMBED EMBED 17 8.5% \u2705 SEC SEC 16 8.0% \u2705 GRAPH GRAPH 15 7.5% \u2705 EVAL EVAL 15 7.5% \u2705 QUERY QUERY 11 5.5% \u2705 Foundation Concepts - Prerequisites FOUND 9 4.5% \u2705 NLP NLP 8 4.0% \u2705 METRIC METRIC 7 3.5% \u2705 LLM LLM 7 3.5% \u2705 TOOL TOOL 3 1.5% \u2139\ufe0f Under"},{"location":"learning-graph/taxonomy-distribution/#visual-distribution","title":"Visual Distribution","text":"<pre><code>CHAT   \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  46 ( 23.0%)\nSEARCH \u2588\u2588\u2588\u2588\u2588\u2588\u2588  28 ( 14.0%)\nRAG    \u2588\u2588\u2588\u2588  18 (  9.0%)\nEMBED  \u2588\u2588\u2588\u2588  17 (  8.5%)\nSEC    \u2588\u2588\u2588\u2588  16 (  8.0%)\nGRAPH  \u2588\u2588\u2588  15 (  7.5%)\nEVAL   \u2588\u2588\u2588  15 (  7.5%)\nQUERY  \u2588\u2588  11 (  5.5%)\nFOUND  \u2588\u2588   9 (  4.5%)\nNLP    \u2588\u2588   8 (  4.0%)\nMETRIC \u2588   7 (  3.5%)\nLLM    \u2588   7 (  3.5%)\nTOOL      3 (  1.5%)\n</code></pre>"},{"location":"learning-graph/taxonomy-distribution/#balance-analysis","title":"Balance Analysis","text":""},{"location":"learning-graph/taxonomy-distribution/#no-over-represented-categories","title":"\u2705 No Over-Represented Categories","text":"<p>All categories are under the 30% threshold. Good balance!</p>"},{"location":"learning-graph/taxonomy-distribution/#i-under-represented-categories-3","title":"\u2139\ufe0f Under-Represented Categories (&lt;3%)","text":"<ul> <li>TOOL (TOOL): 3 concepts (1.5%)</li> <li>Note: Small categories are acceptable for specialized topics</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#category-details","title":"Category Details","text":""},{"location":"learning-graph/taxonomy-distribution/#chat-chat","title":"CHAT (CHAT)","text":"<p>Count: 46 concepts (23.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Chatbot</li> </ol> </li> <li> <ol> <li>Conversational Agent</li> </ol> </li> <li> <ol> <li>Dialog System</li> </ol> </li> <li> <ol> <li>Intent Recognition</li> </ol> </li> <li> <ol> <li>Intent Modeling</li> </ol> </li> <li> <ol> <li>Intent Classification</li> </ol> </li> <li> <ol> <li>Entity Extraction</li> </ol> </li> <li> <ol> <li>Named Entity Recognition</li> </ol> </li> <li> <ol> <li>Entity Type</li> </ol> </li> <li> <ol> <li>Entity Linking</li> </ol> </li> <li> <ol> <li>FAQ</li> </ol> </li> <li> <ol> <li>FAQ Analysis</li> </ol> </li> <li> <ol> <li>Question-Answer Pair</li> </ol> </li> <li> <ol> <li>User Query</li> </ol> </li> <li> <ol> <li>User Intent</li> </ol> </li> <li>...and 31 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#search-search","title":"SEARCH (SEARCH)","text":"<p>Count: 28 concepts (14.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Keyword Search</li> </ol> </li> <li> <ol> <li>Search Index</li> </ol> </li> <li> <ol> <li>Inverted Index</li> </ol> </li> <li> <ol> <li>Reverse Index</li> </ol> </li> <li> <ol> <li>Full-Text Search</li> </ol> </li> <li> <ol> <li>Boolean Search</li> </ol> </li> <li> <ol> <li>Search Query</li> </ol> </li> <li> <ol> <li>Query Parser</li> </ol> </li> <li> <ol> <li>Synonym Expansion</li> </ol> </li> <li> <ol> <li>Thesaurus</li> </ol> </li> <li> <ol> <li>Ontology</li> </ol> </li> <li> <ol> <li>Taxonomy</li> </ol> </li> <li> <ol> <li>Controlled Vocabulary</li> </ol> </li> <li> <ol> <li>Metadata</li> </ol> </li> <li> <ol> <li>Metadata Tagging</li> </ol> </li> <li>...and 13 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#rag-rag","title":"RAG (RAG)","text":"<p>Count: 18 concepts (9.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>External Knowledge</li> </ol> </li> <li> <ol> <li>Public Knowledge Base</li> </ol> </li> <li> <ol> <li>Internal Knowledge</li> </ol> </li> <li> <ol> <li>Private Documents</li> </ol> </li> <li> <ol> <li>Document Corpus</li> </ol> </li> <li> <ol> <li>RAG Pattern</li> </ol> </li> <li> <ol> <li>Retrieval Augmented Generation</li> </ol> </li> <li> <ol> <li>Retrieval Step</li> </ol> </li> <li> <ol> <li>Augmentation Step</li> </ol> </li> <li> <ol> <li>Generation Step</li> </ol> </li> <li> <ol> <li>Context Window</li> </ol> </li> <li> <ol> <li>Prompt Engineering</li> </ol> </li> <li> <ol> <li>System Prompt</li> </ol> </li> <li> <ol> <li>User Prompt</li> </ol> </li> <li> <ol> <li>RAG Limitations</li> </ol> </li> <li>...and 3 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#embed-embed","title":"EMBED (EMBED)","text":"<p>Count: 17 concepts (8.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Word Embedding</li> </ol> </li> <li> <ol> <li>Embedding Vector</li> </ol> </li> <li> <ol> <li>Vector Space Model</li> </ol> </li> <li> <ol> <li>Vector Dimension</li> </ol> </li> <li> <ol> <li>Embedding Model</li> </ol> </li> <li> <ol> <li>Word2Vec</li> </ol> </li> <li> <ol> <li>GloVe</li> </ol> </li> <li> <ol> <li>FastText</li> </ol> </li> <li> <ol> <li>Sentence Embedding</li> </ol> </li> <li> <ol> <li>Contextual Embedding</li> </ol> </li> <li> <ol> <li>Vector Database</li> </ol> </li> <li> <ol> <li>Vector Store</li> </ol> </li> <li> <ol> <li>Vector Index</li> </ol> </li> <li> <ol> <li>Approximate Nearest Neighbor</li> </ol> </li> <li> <ol> <li>FAISS</li> </ol> </li> <li>...and 2 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#sec-sec","title":"SEC (SEC)","text":"<p>Count: 16 concepts (8.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Security</li> </ol> </li> <li> <ol> <li>Authentication</li> </ol> </li> <li> <ol> <li>Authorization</li> </ol> </li> <li> <ol> <li>User Permission</li> </ol> </li> <li> <ol> <li>Role-Based Access Control</li> </ol> </li> <li> <ol> <li>RBAC</li> </ol> </li> <li> <ol> <li>Access Policy</li> </ol> </li> <li> <ol> <li>Data Privacy</li> </ol> </li> <li> <ol> <li>PII</li> </ol> </li> <li> <ol> <li>Personally Identifiable Info</li> </ol> </li> <li> <ol> <li>GDPR</li> </ol> </li> <li> <ol> <li>Data Retention</li> </ol> </li> <li> <ol> <li>Log Storage</li> </ol> </li> <li> <ol> <li>Chat Log</li> </ol> </li> <li> <ol> <li>Logging System</li> </ol> </li> <li>...and 1 more</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#graph-graph","title":"GRAPH (GRAPH)","text":"<p>Count: 15 concepts (7.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>GraphRAG Pattern</li> </ol> </li> <li> <ol> <li>Knowledge Graph</li> </ol> </li> <li> <ol> <li>Graph Database</li> </ol> </li> <li> <ol> <li>Node</li> </ol> </li> <li> <ol> <li>Edge</li> </ol> </li> <li> <ol> <li>Triple</li> </ol> </li> <li> <ol> <li>Subject-Predicate-Object</li> </ol> </li> <li> <ol> <li>RDF</li> </ol> </li> <li> <ol> <li>Graph Query</li> </ol> </li> <li> <ol> <li>OpenCypher</li> </ol> </li> <li> <ol> <li>Cypher Query Language</li> </ol> </li> <li> <ol> <li>Neo4j</li> </ol> </li> <li> <ol> <li>Corporate Nervous System</li> </ol> </li> <li> <ol> <li>Organizational Knowledge</li> </ol> </li> <li> <ol> <li>Knowledge Management</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#eval-eval","title":"EVAL (EVAL)","text":"<p>Count: 15 concepts (7.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Query Frequency</li> </ol> </li> <li> <ol> <li>Frequency Analysis</li> </ol> </li> <li> <ol> <li>Pareto Analysis</li> </ol> </li> <li> <ol> <li>80/20 Rule</li> </ol> </li> <li> <ol> <li>Chatbot Metrics</li> </ol> </li> <li> <ol> <li>KPI</li> </ol> </li> <li> <ol> <li>Key Performance Indicator</li> </ol> </li> <li> <ol> <li>Chatbot Dashboard</li> </ol> </li> <li> <ol> <li>Acceptance Rate</li> </ol> </li> <li> <ol> <li>User Satisfaction</li> </ol> </li> <li> <ol> <li>Response Accuracy</li> </ol> </li> <li> <ol> <li>Chatbot Evaluation</li> </ol> </li> <li> <ol> <li>A/B Testing</li> </ol> </li> <li> <ol> <li>Performance Tuning</li> </ol> </li> <li> <ol> <li>Optimization</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#query-query","title":"QUERY (QUERY)","text":"<p>Count: 11 concepts (5.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Database Query</li> </ol> </li> <li> <ol> <li>SQL Query</li> </ol> </li> <li> <ol> <li>Query Parameter</li> </ol> </li> <li> <ol> <li>Parameter Extraction</li> </ol> </li> <li> <ol> <li>Query Template</li> </ol> </li> <li> <ol> <li>Parameterized Query</li> </ol> </li> <li> <ol> <li>Query Execution</li> </ol> </li> <li> <ol> <li>Query Description</li> </ol> </li> <li> <ol> <li>Natural Language to SQL</li> </ol> </li> <li> <ol> <li>Question to Query Mapping</li> </ol> </li> <li> <ol> <li>Slot Filling</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#foundation-concepts-prerequisites-found","title":"Foundation Concepts - Prerequisites (FOUND)","text":"<p>Count: 9 concepts (4.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Artificial Intelligence</li> </ol> </li> <li> <ol> <li>AI Timeline</li> </ol> </li> <li> <ol> <li>AI Doubling Rate</li> </ol> </li> <li> <ol> <li>Moore's Law</li> </ol> </li> <li> <ol> <li>Natural Language Processing</li> </ol> </li> <li> <ol> <li>Text Processing</li> </ol> </li> <li> <ol> <li>String Matching</li> </ol> </li> <li> <ol> <li>Regular Expressions</li> </ol> </li> <li> <ol> <li>Grep Command</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#nlp-nlp","title":"NLP (NLP)","text":"<p>Count: 8 concepts (4.0%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>NLP Pipeline</li> </ol> </li> <li> <ol> <li>Text Preprocessing</li> </ol> </li> <li> <ol> <li>Text Normalization</li> </ol> </li> <li> <ol> <li>Stemming</li> </ol> </li> <li> <ol> <li>Lemmatization</li> </ol> </li> <li> <ol> <li>Part-of-Speech Tagging</li> </ol> </li> <li> <ol> <li>Dependency Parsing</li> </ol> </li> <li> <ol> <li>Coreference Resolution</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#metric-metric","title":"METRIC (METRIC)","text":"<p>Count: 7 concepts (3.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Search Precision</li> </ol> </li> <li> <ol> <li>Search Recall</li> </ol> </li> <li> <ol> <li>F-Measure</li> </ol> </li> <li> <ol> <li>F1 Score</li> </ol> </li> <li> <ol> <li>Confusion Matrix</li> </ol> </li> <li> <ol> <li>True Positive</li> </ol> </li> <li> <ol> <li>False Positive</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#llm-llm","title":"LLM (LLM)","text":"<p>Count: 7 concepts (3.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Large Language Model</li> </ol> </li> <li> <ol> <li>Transformer Architecture</li> </ol> </li> <li> <ol> <li>Attention Mechanism</li> </ol> </li> <li> <ol> <li>Token</li> </ol> </li> <li> <ol> <li>Tokenization</li> </ol> </li> <li> <ol> <li>Subword Tokenization</li> </ol> </li> <li> <ol> <li>Byte Pair Encoding</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#tool-tool","title":"TOOL (TOOL)","text":"<p>Count: 3 concepts (1.5%)</p> <p>Concepts:</p> <ul> <li> <ol> <li>Team Project</li> </ol> </li> <li> <ol> <li>Capstone Project</li> </ol> </li> <li> <ol> <li>Chatbot Career</li> </ol> </li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#recommendations","title":"Recommendations","text":"<ul> <li>\u2705 Good balance: Categories are reasonably distributed (spread: 21.5%)</li> <li>\u2705 MISC category minimal: Good categorization specificity</li> </ul>"},{"location":"learning-graph/taxonomy-distribution/#educational-use-recommendations","title":"Educational Use Recommendations","text":"<ul> <li>Use taxonomy categories for color-coding in graph visualizations</li> <li>Design curriculum modules based on taxonomy groupings</li> <li>Create filtered views for focused learning paths</li> <li>Use categories for assessment organization</li> <li>Enable navigation by topic area in interactive tools</li> </ul> <p>Report generated by learning-graph-reports/taxonomy_distribution.py</p>"},{"location":"prompts/01-run-the-course-description-analyzer-skill/","title":"Course Description Analyzer Skill","text":"<p>Prompt</p> <p>run the couse-description-analyzer skill </p>"},{"location":"prompts/02-graph-viewer-legend/","title":"Graph Viewer Legend Update","text":"<p>Change the CSS rules in @docs/sims/graph-viewer/local.css so that the legend table  first column has a 100px minimum and the second column has a 50px maximum </p> <p>Updated docs/sims/graph-viewer/local.css with 8 additions        106      font-size: 16px;        107    }        108          109 +  .legend-table td:first-child {        110 +    min-width: 100px;        111 +  }        112 +        113 +  .legend-table td:nth-child(2) {        114 +    max-width: 50px;        115 +  }        116 +        117    .legend-table input[type=\"checkbox\"] {        118      margin-right: 5px;        119    }</p>"},{"location":"sims/","title":"List of MicroSims","text":"<p>Learning Graph Viewer</p>"},{"location":"sims/graph-viewer/","title":"Learning Graph Viewer","text":"<p>Run the Learning Graph Viewer</p> <p>This viewer reads a learning graph data from ../../learning-graph/learning-graph.json:</p> <ol> <li>Search Functionality - Quick node lookup with autocomplete</li> <li>Taxonomy Legend Controls - Filter nodes by category/taxonomy</li> </ol>"},{"location":"sims/graph-viewer/#features","title":"Features","text":""},{"location":"sims/graph-viewer/#search","title":"Search","text":"<ul> <li>Type-ahead search for node names</li> <li>Displays matching results in a dropdown</li> <li>Shows node group/category in results</li> <li>Clicking a result focuses and highlights the node on the graph</li> <li>Only searches visible nodes (respects taxonomy filters)</li> </ul>"},{"location":"sims/graph-viewer/#taxonomy-legend-with-checkboxes","title":"Taxonomy Legend with Checkboxes","text":"<ul> <li>Sidebar legend with all node categories</li> <li>Toggle visibility of entire node groups</li> <li>Color-coded categories matching the graph</li> <li>\"Check All\" and \"Uncheck All\" buttons for bulk operations</li> <li>Collapsible sidebar to maximize graph viewing area</li> </ul>"},{"location":"sims/graph-viewer/#graph-statistics","title":"Graph Statistics","text":"<p>Real-time statistics that update as you filter: - Nodes: Count of visible nodes - Edges: Count of visible edges (both endpoints must be visible) - Orphans: Nodes with no connections (this is an indication that the learning graph needs editing)</p>"},{"location":"sims/graph-viewer/#sample-graph-demo","title":"Sample Graph Demo","text":"<p>The demo includes a Graph Theory learning graph with 10 taxonomy categories:</p> <ul> <li>Foundation (Red) - Core concepts in red boxes that should be pinned to the left</li> <li>Types (Orange) - Graph types</li> <li>Representations (Gold) - Data structures</li> <li>Algorithms (Green) - Basic algorithms</li> <li>Paths (Blue) - Shortest path algorithms</li> <li>Flow (Indigo) - Network flow algorithms</li> <li>Advanced (Violet) - Advanced topics</li> <li>Metrics (Gray) - Centrality measures</li> <li>Spectral (Brown) - Spectral theory</li> <li>ML &amp; Networks (Teal) - Machine learning</li> </ul>"},{"location":"sims/graph-viewer/#usage-tips","title":"Usage Tips","text":"<ol> <li>Hide a category - Uncheck a category in the sidebar to hide all nodes in that group</li> <li>Search within visible nodes - Use search to quickly find specific concepts among visible nodes</li> <li>Focus on a topic - Uncheck all categories, then check only the ones you want to study</li> <li>Collapse sidebar - Click the menu button (\u2630) to hide the sidebar and expand the graph view</li> <li>Find orphans - Check the statistics to see if any nodes lack connections</li> </ol>"},{"location":"sims/graph-viewer/#implementation-notes","title":"Implementation Notes","text":"<p>This viewer follows the standard vis.js architectural patterns:</p> <ul> <li>Uses <code>vis.DataSet</code> for nodes and edges</li> <li>Implements node <code>hidden</code> property for filtering</li> <li>Combines separate search and legend features</li> <li>Updates statistics dynamically based on visibility</li> <li>Maintains consistent styling across features</li> </ul>"},{"location":"sims/graph-viewer/#use-cases","title":"Use Cases","text":"<ul> <li>Course planning - Filter by topic area to design lesson sequences</li> <li>Concept exploration - Search for specific concepts and see their dependencies</li> <li>Gap analysis - Use orphan count to identify disconnected concepts</li> <li>Progressive learning - Start with foundation concepts, gradually enable advanced topics</li> </ul>"}]}